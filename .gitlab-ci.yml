# ============================================================
# subro_web GitLab CI/CD Pipeline
# Based on project patterns, adapted for blue-green deploy
# ============================================================

stages:
  - lint
  - pre
  - build
  - test
  - scan
  - staging
  - e2e
  - deploy
  - verify
  - release
  - cleanup

default:
  retry:
    max: 2
    when:
      - runner_system_failure
      - stuck_or_timeout_failure
      - api_failure
      - unknown_failure

# -------- Workflow Rules --------
workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_BRANCH == "develop"
    - if: $CI_COMMIT_TAG

# -------- Templates --------
.ssh_setup:
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - |
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > ~/.ssh/id_rsa
      fi
    - chmod 600 ~/.ssh/id_rsa
    - |
      if [ -n "${SSH_HOST_KEY:-}" ]; then
        echo "$SSH_HOST_KEY" >> ~/.ssh/known_hosts
      else
        ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
      fi
    - |
      cat > ~/.ssh/config <<EOF
      Host *
        User $SSH_USER
        Port ${SSH_PORT:-22}
        ServerAliveInterval 10
        ServerAliveCountMax 30
        TCPKeepAlive yes
        ConnectTimeout 30
        ConnectionAttempts 3
        IdentityFile ~/.ssh/id_rsa
      EOF
    - chmod 600 ~/.ssh/config

# ============ LINT STAGE ============
lint_backend:
  retry: 1
  timeout: 5m
  stage: lint
  image: python:3.12-slim
  before_script:
    - export POETRY_VERSION=$(awk '/poetry/ {print $2}' .tool-versions)
    - curl -sSL https://install.python-poetry.org | POETRY_VERSION=$POETRY_VERSION python3 -
    - export PATH="$HOME/.local/bin:$PATH"
    - cd backend
    - poetry install --no-root --only dev
  script:
    - poetry run ruff check .
    - poetry run ruff format --check .
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

lint_frontend:
  retry: 1
  timeout: 5m
  stage: lint
  image: node:20-alpine
  before_script:
    - cd frontend
    - npm ci --prefer-offline
  script:
    - npm run lint
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# -------- Global Variables --------
variables:
  DOCKER_IMAGE_API: $CI_REGISTRY_IMAGE/api:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_API_TEST: $CI_REGISTRY_IMAGE/api-test:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_WORKER: $CI_REGISTRY_IMAGE/worker:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_FRONTEND: $CI_REGISTRY_IMAGE/frontend:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_BACKUP: $CI_REGISTRY_IMAGE/backup:$CI_COMMIT_SHORT_SHA
  DOCKER_TLS_CERTDIR: ""
  GIT_STRATEGY: fetch
  GIT_DEPTH: "20"
  GIT_SUBMODULE_STRATEGY: none
  SSH_OPTS: "-T -o IPQoS=0x00 -o BatchMode=yes -o RequestTTY=no -o TCPKeepAlive=yes -o ServerAliveInterval=10 -o ServerAliveCountMax=30"
  REMOTE_DEPLOY_EXPORTS: |
    export DEPLOY_HOST_DIR="${DEPLOY_HOST_DIR}"
    export ENV_FILE_NAME="${ENV_FILE_NAME}"
    export TRIGGER_SCRIPT="${TRIGGER_SCRIPT}"
    export LOG_FILE="${LOG_FILE}"
    export BACKUP_PREFIX="${BACKUP_PREFIX:-}"
    export DEPLOY_ENV="${DEPLOY_ENV}"
  REMOTE_DEPLOY_SCRIPT: |
    set -euo pipefail

    # Ensure deploy directory exists
    mkdir -p "$${DEPLOY_HOST_DIR}"
    cd "$${DEPLOY_HOST_DIR}"

    # Clone/Fetch repo
    if [ ! -d .git ]; then
      git init .
      git remote add origin "git@${CI_SERVER_HOST}:${CI_PROJECT_PATH}.git"
    fi
    git fetch --depth 1 origin "${CI_COMMIT_SHA}"
    git checkout -f "${CI_COMMIT_SHA}"
    git clean -fd

    # Ensure infra directory exists
    mkdir -p infra
    # Write base secrets and filter out dynamic variables that we will append later
    echo '${PROD_ENV_FILE}' | base64 -d | grep -vE '^(DOCKER_IMAGE_|DEPLOY_COMMIT_SHA|USE_PREBUILT_IMAGES|DEPLOY_ENV)' > "infra/$${ENV_FILE_NAME}"

    # Append CI-specific variables
    {
      echo "DOMAIN_NAME='${SSH_HOST}'"
      echo "WEBAUTHN_RP_ID='${SSH_HOST}'"
      echo "WEBAUTHN_ORIGIN='https://${SSH_HOST}'"
      echo "REGISTRY_PASSWORD='${REGISTRY_PASSWORD}'"
      echo "REGISTRY_USER='${REGISTRY_USER}'"
      echo "CI_REGISTRY='${CI_REGISTRY}'"
      echo "DOCKER_IMAGE_API='${DOCKER_IMAGE_API}'"
      echo "DOCKER_IMAGE_WORKER='${DOCKER_IMAGE_WORKER}'"
      echo "DOCKER_IMAGE_FRONTEND='${DOCKER_IMAGE_FRONTEND}'"
      echo "DOCKER_IMAGE_BACKUP='${DOCKER_IMAGE_BACKUP}'"
      echo "DEPLOY_COMMIT_SHA='${CI_COMMIT_SHA}'"
      echo "BACKUP_PREFIX='$${BACKUP_PREFIX:-}'"
      echo "USE_PREBUILT_IMAGES='1'"
      echo "DEPLOY_ENV='$${DEPLOY_ENV:-production}'"
    } >> "infra/$${ENV_FILE_NAME}"

    chmod 600 "infra/$${ENV_FILE_NAME}"
    chmod +x ./infra/scripts/*.sh

    # Trigger detached deployment
    nohup ./infra/scripts/ci_deploy_trigger.sh \
        "./infra/$${ENV_FILE_NAME}" \
        "./infra/scripts/$${TRIGGER_SCRIPT}" \
        "$${LOG_FILE}" > /dev/null 2>&1 &

    echo "âœ… Deployment triggered in background. Check '$${LOG_FILE}' on server for details."

# -------- Docker Helper (Shell + Docker executor compatible) --------
.default-docker:
  image: docker:25
  services:
    - name: docker:25-dind
      command: ["--tls=false"]
  variables:
    DOCKER_HOST: tcp://docker:2375
  before_script:
    - set -euo pipefail
    # Auto-detect Shell executor and use local Docker socket instead of dind
    - |
      if [ -S /var/run/docker.sock ]; then
        echo "Shell executor detected - using local Docker socket"
        unset DOCKER_HOST
        export DOCKER_HOST="unix:///var/run/docker.sock"
      else
        echo "Docker executor detected - using dind service"
      fi
    - |
      for i in $(seq 1 3); do
        if echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"; then
          echo "Docker login successful!"
          if [ -n "${CI_DEPENDENCY_PROXY_SERVER:-}" ] && [ "$CI_DEPENDENCY_PROXY_SERVER" != "$CI_REGISTRY" ]; then
            echo "$CI_DEPENDENCY_PROXY_PASSWORD" | docker login -u "$CI_DEPENDENCY_PROXY_USER" --password-stdin "$CI_DEPENDENCY_PROXY_SERVER" || echo "Warning: Dependency Proxy login failed"
          fi
          break
        fi
        echo "Docker login failed (attempt $i/3). Retrying in 5 seconds..."
        sleep 5
      done
    - |
      if [ -n "${DOCKERHUB_USER:-}" ] && [ -n "${DOCKERHUB_TOKEN:-}" ]; then
        for i in $(seq 1 3); do
          if echo "$DOCKERHUB_TOKEN" | docker login -u "$DOCKERHUB_USER" --password-stdin; then
            echo "Docker Hub login successful!"
            break
          fi
          echo "Docker Hub login failed (attempt $i/3). Retrying in 5 seconds..."
          sleep 5
        done
      else
        echo "Docker Hub credentials not set; skipping Docker Hub login."
      fi

# -------- DEBUG (optional) --------
ci_debug_env:
  stage: pre
  image: alpine:3.20
  rules:
    - when: always
  allow_failure: true
  script:
    - 'echo "Pipeline: $CI_PIPELINE_SOURCE | Branch: $CI_COMMIT_BRANCH | SHA: $CI_COMMIT_SHORT_SHA"'

# -------- VERSION BUMP --------
# NOTE: Versioning is handled by semantic-release in the release stage .
# It analyzes conventional commits to determine version bumps:
#   - fix: -> PATCH
#   - feat: -> MINOR
#   - feat!: or BREAKING CHANGE: -> MAJOR

# ============ BUILD STAGE ============
.build_template:
  stage: build
  extends: .default-docker
  variables:
    DOCKER_BUILDKIT: "1"
  script:
    - |
      set -euo pipefail
      export POETRY_VERSION=$(awk '/poetry/ {print $2}' .tool-versions)
      echo "Initializing Buildx builder..."
      docker buildx ls
      # Force create or use existing
      docker buildx create --name builder --driver docker-container --use --bootstrap || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        --build-arg POETRY_VERSION="$POETRY_VERSION" \
        --build-arg CACHE_BUST="$CI_PIPELINE_ID" \
        ${BUILD_ARGS:-} \
        "$CONTEXT"
  rules:
    - when: on_success
  retry: 1

build_api:
  extends: .build_template
  timeout: 30m
  needs: []
  variables:
    SERVICE: api
    DOCKER_IMAGE: $DOCKER_IMAGE_API
    TARGET: production-api
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
    BUILD_ARGS: --build-arg BASE_IMAGE=public.ecr.aws/docker/library/ubuntu:24.04

build_api_test:
  extends: .build_template
  timeout: 30m
  variables:
    SERVICE: api-test
    DOCKER_IMAGE: $DOCKER_IMAGE_API_TEST
    TARGET: ci-test
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
    BUILD_ARGS: --build-arg BASE_IMAGE=public.ecr.aws/docker/library/ubuntu:24.04

build_worker:
  extends: .build_template
  timeout: 30m
  needs: []
  variables:
    SERVICE: worker
    DOCKER_IMAGE: $DOCKER_IMAGE_WORKER
    TARGET: production-worker
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
    BUILD_ARGS: --build-arg BASE_IMAGE=public.ecr.aws/docker/library/ubuntu:24.04

build_frontend:
  extends: .build_template
  timeout: 30m
  needs: []
  variables:
    SERVICE: frontend
    DOCKER_IMAGE: $DOCKER_IMAGE_FRONTEND
    TARGET: production
    DOCKERFILE: frontend/Dockerfile
    CONTEXT: frontend
    BUILD_ARGS: >-
      --build-arg BASE_IMAGE_NODE=public.ecr.aws/docker/library/node:20-alpine
      --build-arg BASE_IMAGE_NGINX=public.ecr.aws/docker/library/nginx:stable-alpine
      --build-arg VITE_API_BASE_URL=${VITE_API_BASE_URL:-/api/v1}
      --build-arg VITE_WS_BASE_URL=${VITE_WS_BASE_URL:-/api/v1}

build_backup:
  extends: .build_template
  timeout: 30m
  variables:
    SERVICE: backup
    DOCKER_IMAGE: $DOCKER_IMAGE_BACKUP
    TARGET: production
    DOCKERFILE: infra/docker/backup/Dockerfile
    CONTEXT: .
    BUILD_ARGS: --build-arg BASE_IMAGE=public.ecr.aws/docker/library/alpine:3.20

# ============ TEST STAGE ============
backend_tests:
  stage: test
  extends: .default-docker
  needs: [build_api_test]
  timeout: 40m
  interruptible: false
  variables:
    POSTGRES_USER: "${CI_TEST_POSTGRES_USER}"
    POSTGRES_PASSWORD: "${CI_TEST_POSTGRES_PASSWORD}"
    POSTGRES_DB: "${CI_TEST_POSTGRES_DB}"
    POSTGRES_DB_TEST: "${CI_TEST_POSTGRES_DB_TEST}"
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - when: on_success
  script:
    - |
      if command -v apk &> /dev/null; then
        apk add --no-cache bash
      fi
    - |
      set -euo pipefail
      # Define images using AWS ECR Public Gallery (reliable mirror)
      IMAGE_POSTGRES="public.ecr.aws/docker/library/postgres:16-alpine"
      IMAGE_REDIS="public.ecr.aws/docker/library/redis:7-alpine"
      # Use alpine with curl to avoid Docker Hub rate limits
      IMAGE_CURL="public.ecr.aws/docker/library/alpine:3.20"

      DEPENDENCIES="$DOCKER_IMAGE_API_TEST $IMAGE_POSTGRES $IMAGE_REDIS $IMAGE_CURL"
      for img in $DEPENDENCIES; do
        echo "Pulling $img..."
        for i in $(seq 1 3); do
          if docker pull "$img"; then
            echo "Successfully pulled $img"
            # Tag back to standard name if it was pulled from mirror
            if [[ "$img" == *"public.ecr.aws"* ]]; then
               # Extract base name e.g. public.ecr.aws/docker/library/postgres:16-alpine -> postgres:16-alpine
               STANDARD_NAME=$(echo "$img" | sed 's|public.ecr.aws/docker/library/||')
               docker tag "$img" "$STANDARD_NAME"
               echo "Tagged $img as $STANDARD_NAME"
            fi
            break
          fi
          if [ $i -eq 3 ]; then
            echo "ERROR: Failed to pull $img after 3 attempts."
            exit 1
          fi
          echo "Docker pull failed (attempt $i/3). Retrying in 5 seconds..."
          sleep 5
        done
      done
    - docker network create testnet || true
    # Start dependencies (Production-like setup)
    - docker run -d --name db --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB postgres:16-alpine
    - docker run --rm --network testnet --entrypoint python $DOCKER_IMAGE_API_TEST -c "import kombu, redis; print('DEBUG VERSIONS kombu', kombu.__version__, 'redis', redis.__version__)"
    - docker run -d --name db_test --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB_TEST postgres:16-alpine
    - docker run -d --name redis --network testnet redis:7-alpine

    # Wait for databases to be ready
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet postgres:16-alpine sh -c "pg_isready -h db -U $POSTGRES_USER && pg_isready -h db_test -U $POSTGRES_USER"; then
          echo "Databases ready!"
          break
        fi
        echo "Waiting for Databases ($i/30)..." && sleep 2
      done

    # Start API with env file from repo, override network-specific vars
    - |
      docker run -d --name api --network testnet \
        --env-file backend/.env.test \
        -e DATABASE_URL="postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}" \
        -e DB_HOST="db" \
        -e POSTGRES_SERVER="db" \
        -e REDIS_HOST="redis" \
        -e CELERY_BROKER_URL="redis://redis:6379/0" \
        -e CELERY_RESULT_BACKEND="redis://redis:6379/1" \
        -e REDIS_PUBSUB_URL="redis://redis:6379/2" \
        -e TEST_DATABASE_HOST="db_test" \
        -e TEST_DATABASE_PORT="5432" \
        -e TEST_API_BASE_URL="http://localhost:8000" \
        -e TEST_WS_BASE_URL="ws://localhost:8000" \
        -v "$(pwd)/backend/tests:/app/tests" \
        "$DOCKER_IMAGE_API_TEST"

    # Wait for API to be healthy
    - |
      docker run --rm --network testnet alpine:3.20 sh -c '
        apk add --no-cache -q curl
        for i in $(seq 1 30); do
          if curl -fsS http://api:8000/health >/dev/null 2>&1; then
            echo "API healthy!"
            exit 0
          fi
          echo "Waiting for API ($i/30)..." && sleep 2
        done
        exit 1'

    - docker exec api poetry run pytest -p no:cacheprovider tests/
  after_script:
    - docker logs api 2>&1 | tail -50 || true
    - docker rm -f api db db_test redis 2>/dev/null || true
    - docker network rm testnet 2>/dev/null || true

frontend_tests:
  stage: test
  image: node:20-alpine
  needs: []
  timeout: 30m
  variables:
    NPM_CONFIG_CACHE: "$CI_PROJECT_DIR/frontend/.npm"
    VITE_API_BASE_URL: "${VITE_API_BASE_URL:-http://localhost:8000/api/v1}"
    VITE_WS_BASE_URL: "${VITE_WS_BASE_URL:-ws://localhost:8000/api/v1}"
    VITE_API_PROXY_TARGET: "${VITE_API_PROXY_TARGET:-http://localhost:8000}"
  cache:
    key:
      files:
        - frontend/package-lock.json
    policy: pull-push
    paths:
      - frontend/node_modules/
      - frontend/.npm/
  before_script:
    - set -euo pipefail
    - cd frontend
    - npm config set cache .npm
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - when: on_success
  script:
    - npm ci --prefer-offline --no-audit --no-fund
    - npm test -- --maxWorkers=4 --coverage=false

# ============ SCAN STAGE ============
scan_image_vulns:
  stage: scan
  image:
    name: aquasec/trivy:0.58.0
    entrypoint: [""]
  needs: [build_api, build_worker, build_frontend, build_backup]
  timeout: 30m
  variables:
    TRIVY_SEVERITY: "HIGH,CRITICAL"
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        TRIVY_SEVERITY: "CRITICAL"
    - when: on_success
  cache:
    key: trivy-cache-v3
    paths: [.trivycache/]
  script:
    - |
      set -euo pipefail

      # Auto-detect Shell executor
      USE_DOCKER=false
      if [ -S /var/run/docker.sock ] && ! command -v trivy &> /dev/null; then
        echo "Shell executor detected - running trivy via Docker"
        USE_DOCKER=true
        # Docker login for registry access
        echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"
        # Ensure cache directory exists with correct ownership
        mkdir -p .trivycache
      else
        export TRIVY_USERNAME="$CI_REGISTRY_USER"
        export TRIVY_PASSWORD="$CI_REGISTRY_PASSWORD"
      fi

      # Scan images (trivy is fast enough that sequential is fine)
      for img in "$DOCKER_IMAGE_API" "$DOCKER_IMAGE_FRONTEND" "$DOCKER_IMAGE_WORKER" "$DOCKER_IMAGE_BACKUP"; do
        echo "Scanning $img..."
        # Retry scan up to 3 times to handle eventual consistency or network flakes
        for attempt in 1 2 3; do
          SCAN_SUCCESS=false

          if [ "$USE_DOCKER" = "true" ]; then
            # Pull image first, then scan locally
            docker pull "$img" || true
            # Run as current user to avoid permission issues with cache
            if docker run --rm \
              --user "$(id -u):$(id -g)" \
              -v /var/run/docker.sock:/var/run/docker.sock \
              -v "$(pwd)/.trivycache:/.cache/trivy" \
              aquasec/trivy:0.58.0 image --exit-code 1 \
              --cache-dir /.cache/trivy \
              --db-repository ghcr.io/aquasecurity/trivy-db:2 \
              --severity "$TRIVY_SEVERITY" --timeout 5m \
              --skip-files "/app/secrets/google-api-config.json" \
              --scanners vuln \
              "$img"; then
              SCAN_SUCCESS=true
            fi
          else
            if trivy image --cache-dir .trivycache --exit-code 1 \
              --db-repository ghcr.io/aquasecurity/trivy-db:2 \
              --severity "$TRIVY_SEVERITY" --timeout 5m \
              --skip-files "/app/secrets/google-api-config.json" \
              --scanners vuln \
              "$img"; then
              SCAN_SUCCESS=true
            fi
          fi

          if [ "$SCAN_SUCCESS" = "true" ]; then
            echo "âœ“ Scan successful for $img"
            break
          fi
          if [ $attempt -eq 3 ]; then
            echo "ERROR: Failed to scan $img after 3 attempts"
            exit 1
          fi
          echo "Scan failed (attempt $attempt/3). Retrying in 10s..."
          sleep 10
        done
      done
      echo "All image scans passed!"
  allow_failure: false
  retry: 1

sast_scan:
  stage: scan
  needs: []
  image:
    name: semgrep/semgrep:latest
    entrypoint: [""]
  timeout: 30m
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - if: '$CI_COMMIT_BRANCH == "main"'
    - when: on_success
  script:
    - |
      echo "Running Semgrep SAST scan..."

      # Define semgrep command
      SEMGREP_CMD="semgrep scan \
        --config p/default \
        --config p/python \
        --config p/typescript \
        --sarif -o /src/semgrep.sarif \
        --error \
        --exclude='*.lock' \
        --exclude='**/package-lock.json' \
        --exclude='**/poetry.lock' \
        --exclude='**/node_modules' \
        --exclude='**/.venv' \
        --exclude='**/dist' \
        --exclude='**/build' \
        --exclude='**/alembic/versions' \
        --jobs 4 \
        --max-target-bytes 500000 \
        --timeout 120 \
        /src"

      # Auto-detect Shell executor and run via Docker
      if [ -S /var/run/docker.sock ] && ! command -v semgrep &> /dev/null; then
        echo "Shell executor detected - running semgrep via Docker"
        docker run --rm -v "$(pwd):/src" semgrep/semgrep:latest $SEMGREP_CMD
        # Move artifact to expected location
        mv semgrep.sarif semgrep.sarif 2>/dev/null || true
      else
        # Running in Docker executor with semgrep image
        SEMGREP_CMD="${SEMGREP_CMD//\/src\//}"
        SEMGREP_CMD="${SEMGREP_CMD//\/src/.}"
        SEMGREP_CMD="${SEMGREP_CMD//-o \/src\/semgrep.sarif/-o semgrep.sarif}"
        eval $SEMGREP_CMD
      fi

      echo "âœ“ Semgrep scan complete - no findings"
  artifacts:
    reports:
      sast: semgrep.sarif
    expire_in: 2 days
    when: always
  allow_failure: false
  retry: 1

secret_scan:
  stage: scan
  needs: []
  timeout: 30m
  image:
    name: aquasec/trivy:0.58.0
    entrypoint: [""]
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - when: on_success
  script:
    - |
      # Auto-detect Shell executor and run via Docker
      if [ -S /var/run/docker.sock ] && ! command -v trivy &> /dev/null; then
        echo "Shell executor detected - running trivy via Docker"
        docker run --rm -v "$(pwd):/src" -w /src aquasec/trivy:0.58.0 \
          fs --scanners secret \
          --skip-dirs .git,__pycache__,.venv,venv,node_modules,.ruff_cache,.pytest_cache \
          --exit-code 1 --timeout 5m .
      else
        trivy fs --scanners secret \
          --skip-dirs .git,__pycache__,.venv,venv,node_modules,.ruff_cache,.pytest_cache \
          --exit-code 1 --timeout 5m .
      fi
  allow_failure: false
  retry: 1

# ============ STAGING STAGE ============
deploy_staging:
  stage: staging
  image: alpine:3.20
  resource_group: staging
  timeout: 30m
  allow_failure: false
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
    DEPLOY_HOST_DIR: /opt/subro_web_staging
    ENV_FILE_NAME: .env.staging
    TRIGGER_SCRIPT: deploy_staging.sh
    LOG_FILE: last_deploy.log
    BACKUP_PREFIX: staging_
    DEPLOY_ENV: staging
  needs:
    - job: backend_tests
      artifacts: false
    - job: frontend_tests
      artifacts: false
    - job: build_api
      artifacts: false
    - job: build_worker
      artifacts: false
    - job: build_frontend
      artifacts: false
    - job: build_backup
      artifacts: false
    - job: scan_image_vulns
      artifacts: false
    - job: sast_scan
      artifacts: false
    - job: secret_scan
      artifacts: false
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  extends: .ssh_setup
  script:
    - |
      set -euo pipefail

      echo "=== Starting Consolidated Staging Deployment ==="
      ssh $SSH_OPTS "$SSH_HOST" bash -s <<DEPLOY_STAGE
        set -euo pipefail

        # 0. Ensure server-side SSH keepalives are configured (idempotent)
        if [ ! -f /etc/ssh/sshd_config.d/keepalive.conf ]; then
          printf "%s\n" "# CI/CD keepalives" "ClientAliveInterval 15" "ClientAliveCountMax 10" "TCPKeepAlive yes" > /etc/ssh/sshd_config.d/keepalive.conf &&
          (systemctl reload ssh 2>/dev/null || systemctl reload sshd 2>/dev/null || true) &&
          echo "SSH keepalives configured";
        fi

        # 1. Verify Docker images exist before deploying
        echo ""
        echo "=== Verifying Docker Images ==="

        # Start background keepalive to prevent SSH timeout during slow docker operations
        (while true; do echo -n " "; sleep 5; done) &
        KEEPALIVE_PID=\$!
        trap "kill \$KEEPALIVE_PID 2>/dev/null || true" EXIT

        # Docker login with retry (handles registry timeouts)
        for i in 1 2 3; do
          if echo "${REGISTRY_PASSWORD}" | docker login -u "${REGISTRY_USER}" --password-stdin "${CI_REGISTRY}" 2>/dev/null; then
            echo ""
            echo 'Docker login successful'
            break
          fi
          if [ \$i -eq 3 ]; then
            kill \$KEEPALIVE_PID 2>/dev/null
            echo ""
            echo 'âŒ Docker login failed after 3 attempts'
            exit 1
          fi
          echo ""
          echo "Docker login attempt \$i failed, retrying in 5s..."
          sleep 5
        done

        # Skip slow manifest verification - images were just pushed by build stage
        # If they somehow don't exist, the docker pull in deploy will fail fast
        echo 'Images built and pushed by CI - skipping slow manifest verification'
        echo "Will deploy: ${DOCKER_IMAGE_API}"
        echo "Will deploy: ${DOCKER_IMAGE_FRONTEND}"

        # Kill keepalive before continuing
        kill \$KEEPALIVE_PID 2>/dev/null || true
        echo ""

        docker logout "${CI_REGISTRY}"

        # Export deploy variables for REMOTE_DEPLOY_SCRIPT
        $REMOTE_DEPLOY_EXPORTS
        $REMOTE_DEPLOY_SCRIPT
      DEPLOY_STAGE

verify_staging:
  retry: 1
  timeout: 30m
  stage: staging
  image: alpine:3.20
  allow_failure:
  needs: [deploy_staging]
  resource_group: staging
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  extends: .ssh_setup
  script:
    - apk add --no-cache curl bind-tools
    - |
      # Resolve SSH_HOST to IP and add staging subdomain to /etc/hosts
      SERVER_IP=$(nslookup "$SSH_HOST" | awk '/^Address: / { print $2 }' | tail -1)
      if [ -z "$SERVER_IP" ]; then
        echo "ERROR: Could not resolve IP for $SSH_HOST"
        exit 1
      fi
      echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
      echo "Added /etc/hosts entry: $SERVER_IP staging.$SSH_HOST"
    - |
      echo "Verifying staging deployment..."
      echo ""
      # Quick container status check
      echo "ðŸ“¦ Checking container status..."
      ssh "$SSH_HOST" \
        "docker ps --filter 'name=subro.*staging' --format '  â€¢ {{.Names}}: {{.Status}}' || echo '  âš ï¸  Could not check containers'"
      echo ""

      # Increased retry window for staging container startup
      MAX_ATTEMPTS=24  # 24 * 10s = 4 minutes
      DEPLOY_MARKER_PATH="${STAGING_DEPLOY_DIR}/infra/.deploy_sha"
      SHORT_SHA="${CI_COMMIT_SHA:0:8}"

      # Initial wait for containers to start before polling
      echo "â³ Waiting 30s for containers to initialize..."
      sleep 30

      echo ""
      echo "ðŸ” Verifying deployment (commit: $SHORT_SHA)..."
      echo ""

      for attempt in $(seq 1 $MAX_ATTEMPTS); do
        MARKER_OK=false
        API_OK=false

        # Check deploy marker
        DEPLOYED_SHA=$(ssh "$SSH_HOST" \
          "cat '$DEPLOY_MARKER_PATH' 2>/dev/null || true")

        if [ "$DEPLOYED_SHA" = "$CI_COMMIT_SHA" ]; then
          MARKER_OK=true
        fi

        # Check API response
        HTTP_CODE=$(curl -k -s -o /dev/null -w "%{http_code}" --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "000")
        RESPONSE=$(curl -k -s --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "")

        if echo "$RESPONSE" | grep -qE '"(version|message)"'; then
          API_OK=true
        fi

        # Success condition
        if [ "$MARKER_OK" = "true" ] && [ "$API_OK" = "true" ]; then
          VERSION=$(echo "$RESPONSE" | sed -n 's/.*"version":"\([^"]*\)".*/\1/p')
          if [ -z "$VERSION" ]; then VERSION="unknown"; fi
          echo ""
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
          echo "â”‚  âœ… Staging deployment verified successfully!         â”‚"
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
          printf "â”‚  â€¢ %-10s %-41s â”‚\n" "Commit:" "$SHORT_SHA"
          printf "â”‚  â€¢ %-10s %-41s â”‚\n" "Version:" "$VERSION"
          printf "â”‚  â€¢ %-10s %-41s â”‚\n" "API:" "https://staging.$SSH_HOST/api/v1/"
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
          exit 0
        fi

        # Show progress dot for non-verbose output
        if [ $attempt -eq 1 ]; then
          printf "   Waiting for deployment"
        fi
        printf "."

        # Only show detailed status every 6th attempt or on specific errors
        if [ $((attempt % 6)) -eq 0 ]; then
          echo ""
          if [ "$MARKER_OK" != "true" ]; then
            echo "   â³ Deploy marker: waiting (${DEPLOYED_SHA:0:8}... â†’ $SHORT_SHA)"
          fi
          if [ "$HTTP_CODE" = "502" ] || [ "$HTTP_CODE" = "503" ]; then
            echo "   â³ API: starting (HTTP $HTTP_CODE)"
          elif [ "$API_OK" != "true" ]; then
            echo "   â³ API: not ready yet"
          fi
          printf "   Waiting"
        fi

        sleep 10
      done

      echo ""
      echo ""
      echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
      echo "â”‚  âŒ Staging verification failed                     â”‚"
      echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      echo ""
      echo "Possible causes:"
      echo "  â€¢ Staging containers failed to start"
      echo "  â€¢ Caddy not configured for staging.secure.go.ro"
      echo "  â€¢ Network/firewall issues"
      echo ""
      echo "=== SERVER-SIDE DEPLOYMENT LOG (last_deploy.log) ==="
      ssh "$SSH_HOST" "cat ${STAGING_DEPLOY_DIR}/last_deploy.log || echo 'Log file not found'"
      echo ""
      echo "=== API CONTAINER LOGS (subro_staging-api-1) ==="
      ssh "$SSH_HOST" "docker logs subro_staging-api-1 --tail 100 || echo 'Container not found'"
      echo ""
      echo "Troubleshooting:"
      echo "  1. SSH: docker ps -a | grep staging"
      echo "  2. Logs: docker logs subro_staging-api-1 --tail 50"
      echo "  3. Test: curl http://localhost:8000/api/v1/"
      exit 1
  environment:
    name: staging
    url: https://staging.$SSH_HOST/

e2e_tests:
  stage: e2e
  image: mcr.microsoft.com/playwright:v1.57.0-jammy
  needs: [verify_staging]
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /^chore\(release\):/'
      when: never
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    E2E_BASE_URL: https://staging.$SSH_HOST
  script:
    - |
      # Resolve SSH_HOST to IP with validation (consistent with verify_staging)
      SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1)
      if [ -z "$SERVER_IP" ]; then
        echo "ERROR: Could not resolve IP for $SSH_HOST"
        exit 1
      fi
      echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
      echo "Added /etc/hosts entry: $SERVER_IP staging.$SSH_HOST"
    - cd frontend
    - npm ci --prefer-offline
    - npx playwright install --with-deps chromium
    - npx playwright test --reporter=html
  artifacts:
    when: always
    paths:
      - frontend/playwright-report/
    expire_in: 7 days

# Cleanup staging after E2E tests (ephemeral staging)
cleanup_staging:
  retry: 1
  timeout: 30m
  stage: e2e
  image: alpine:3.20
  resource_group: staging
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  needs:
    - job: e2e_tests
      artifacts: false
      optional: true
    - job: deploy_staging
      artifacts: false
      # Required - cleanup only if deployment succeeded
    - job: verify_staging
      artifacts: false
      # Required - cleanup only if verification succeeded
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  extends: .ssh_setup
  script:
    - |
      ssh $SSH_OPTS -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" bash -s <<CLEANUP_SCRIPT
        set -euo pipefail

        # ANSI color codes for inline script
        BLUE='\033[0;34m'
        GREEN='\033[0;32m'
        NC='\033[0m'

        log() { echo -e "\${BLUE}[\$(date +'%Y-%m-%dT%H:%M:%S%z')]\${NC} \$*"; }
        success() { echo -e "\${GREEN}[\$(date +'%Y-%m-%dT%H:%M:%S%z')] âœ“ \$*\${NC}"; }

        STAGING_DIR="${STAGING_DEPLOY_DIR:-/opt/subro_web_staging}"

        log "Starting cleanup for staging environment..."
        log "Target directory: \$STAGING_DIR"

        if [ -d "\$STAGING_DIR" ]; then
          cd "\$STAGING_DIR"

          # Stop staging containers (app stack) and remove volumes for true ephemeral state
          docker compose -p subro_staging \
            --env-file infra/.env.staging \
            -f infra/docker/compose.prod.yml \
            down --remove-orphans --volumes 2>/dev/null || true

          success 'Staging environment stopped and cleaned'
        else
          log 'Staging directory not found, nothing to clean up'
        fi
      CLEANUP_SCRIPT
  allow_failure: true

# ============ DEPLOY STAGE ============
.deploy_template:
  stage: deploy
  image: alpine:3.20
  extends: .ssh_setup
  resource_group: $CI_ENVIRONMENT_NAME
  needs:
    - job: backend_tests
      artifacts: false
    - job: scan_image_vulns
      artifacts: false
    - job: sast_scan
      artifacts: false
    - job: secret_scan
      artifacts: false
  retry: 1

deploy_prod:
  extends: .deploy_template
  needs:
    - job: build_api
      artifacts: false
    - job: build_worker
      artifacts: false
    - job: build_frontend
      artifacts: false
    - job: build_backup
      artifacts: false
  environment:
    name: production
    url: https://$SSH_HOST/
    deployment_tier: production
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE =~ /^chore\(release\):/ && $CI_COMMIT_MESSAGE !~ /\[skip ci\]/'
      when: on_success
    - when: never
  variables:
    DEPLOY_HOST_DIR: /opt/subro_web
    ENV_FILE_NAME: .env.prod
    TRIGGER_SCRIPT: blue_green_deploy.sh
    LOG_FILE: last_deploy_prod.log
    BACKUP_PREFIX: ""
    DEPLOY_ENV: production
  script:
    - |
      ssh $SSH_OPTS \
          -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" bash -s <<DEPLOY_PROD
        $REMOTE_DEPLOY_EXPORTS
        $REMOTE_DEPLOY_SCRIPT
      DEPLOY_PROD

# ============ VERIFY STAGE ============
verify_prod:
  retry: 1
  timeout: 30m
  stage: verify
  image: alpine:3.20
  needs:
    - job: deploy_prod
  environment:
    name: production
    url: https://$SSH_HOST/
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE =~ /^chore\(release\):/ && $CI_COMMIT_MESSAGE !~ /\[skip ci\]/'
      when: on_success
  script:
    - apk add --no-cache curl jq openssh-client
    - |
      # Load expected version
      if [ -f version.env ]; then
        source version.env
        EXPECTED_VERSION="${NEW_APP_VERSION:-}"
      else
        echo "Warning: version.env not found. Skipping strict version check."
        EXPECTED_VERSION=""
      fi
    - |-
      # SSH setup for remote diagnostics
      mkdir -p ~/.ssh && chmod 700 ~/.ssh
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > ~/.ssh/id_rsa
      fi
      chmod 600 ~/.ssh/id_rsa
      ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts 2>/dev/null

      echo ""
      echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
      echo "â”‚  ðŸ” Production Verification                                 â”‚"
      echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      echo ""
      echo "ðŸ“¦ Checking container status on $SSH_HOST..."
      ssh -o BatchMode=yes "$SSH_USER@$SSH_HOST" \
        "docker ps --filter 'name=-api-1' --filter 'name=-frontend-1' --format '  â€¢ {{.Names}}: {{.Status}}'" || echo "  âš ï¸  Could not check containers"
      echo ""

      echo "ðŸ”€ Checking Caddy routing..."
      ACTIVE_COLOR=$(ssh -o BatchMode=yes "$SSH_USER@$SSH_HOST" \
        "grep -m1 'reverse_proxy' /opt/subro_web/infra/docker/Caddyfile.prod 2>/dev/null | grep -oE '(blue|green)' | head -1" || echo "unknown")
      echo "  â€¢ Active color: $ACTIVE_COLOR"
      echo ""

      echo "ðŸŒ Verifying API health..."
      if [ -n "$EXPECTED_VERSION" ]; then
        echo "  â€¢ Expecting Version: $EXPECTED_VERSION"
      fi

      MAX_ATTEMPTS=12
      for i in $(seq 1 $MAX_ATTEMPTS); do
        # Try main health endpoint first
        RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/api/v1/" 2>&1 || true)

        if echo "$RESPONSE" | grep -q '"version"'; then
          VERSION=$(echo "$RESPONSE" | jq -r '.version // "unknown"' 2>/dev/null || echo "unknown")

          # Strict version check
          if [ -n "$EXPECTED_VERSION" ]; then
            # Clean up versions for comparison (handle potential suffixes if mismatching)
            # Just do direct string comparison first
            if [[ "$VERSION" == *"$EXPECTED_VERSION"* ]] || [[ "$EXPECTED_VERSION" == *"$VERSION"* ]]; then
                 CHECK_RESULT="âœ… MATCH"
            else
                 CHECK_RESULT="âŒ MISMATCH (Expected $EXPECTED_VERSION)"
            fi
          else
            CHECK_RESULT="âš ï¸  NO CHECK"
          fi

          if [[ "$CHECK_RESULT" == *"âœ…"* ]] || [[ "$CHECK_RESULT" == *"NO CHECK"* ]]; then
              echo ""
              echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
              echo "â”‚  âœ… Production verified successfully!                       â”‚"
              echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
              printf "â”‚  â€¢ %-12s %-44s â”‚\n" "Version:" "$VERSION $CHECK_RESULT"
              printf "â”‚  â€¢ %-12s %-44s â”‚\n" "Color:" "$ACTIVE_COLOR"
              printf "â”‚  â€¢ %-12s %-44s â”‚\n" "URL:" "https://$SSH_HOST/"
              echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
              exit 0
          fi

          echo "  â³ Version mismatch: Got $VERSION, Expected $EXPECTED_VERSION. Retrying..."
        fi

        echo "  â³ Attempt $i/$MAX_ATTEMPTS - waiting for API..."
        sleep 5
      done

      echo ""
      echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
      echo "â”‚  âŒ Production verification failed                          â”‚"
      echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      echo ""
      echo "Last response: $RESPONSE"
      if [ -n "$EXPECTED_VERSION" ]; then
        echo "Expected Version: $EXPECTED_VERSION"
      fi
      echo ""
      echo "Troubleshooting:"
      echo "  1. Check container logs: docker logs \${ACTIVE_COLOR}-api-1 --tail 50"
      echo "  2. Check Caddy logs: docker logs infra-caddy-1 --tail 50"
      echo "  3. Verify Caddyfile.prod routing"
      exit 1

# ============ RELEASE STAGE ============
# Semantic Release - analyzes conventional commits to determine version bump
semantic_release:
  retry: 1
  timeout: 10m
  stage: release
  image: node:20-alpine
  needs:
    # Remove dependency on verify_prod because in the first pipeline (pre-release),
    # verify_prod is skipped. We depend on E2E/Tests instead to ensure quality.
    - job: e2e_tests
      artifacts: false
  variables:
    GIT_STRATEGY: clone
    GIT_DEPTH: "0"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE !~ /chore\(release\)/ && $CI_COMMIT_MESSAGE !~ /\[skip ci\]/'
      when: on_success
    - when: never
  before_script:
    - apk add --no-cache git openssh-client bash
    - npm ci --prefer-offline
    - mkdir -p ~/.ssh && chmod 700 ~/.ssh
    - |-
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > ~/.ssh/id_rsa
      fi
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan $CI_SERVER_HOST >> ~/.ssh/known_hosts 2>/dev/null
    - git config --global user.email "ci-bot@gitlab.com"
    - git config --global user.name "CI Bot"
    - git remote set-url origin git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git
  script:
    - |-
      echo ""
      echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
      echo "â”‚  ðŸš€ Running Semantic Release                                â”‚"
      echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      echo ""
      npx semantic-release


# ============ CLEANUP STAGE ============
cleanup_registry:
  retry: 1
  timeout: 15m
  stage: cleanup
  image: alpine:3.20
  variables:
    KEEP_TAGS: "10" # Number of recent tags to keep per repository
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true
    - if: '$CI_COMMIT_BRANCH == "develop"'
      when: on_success
      allow_failure: true
  script:
    - apk add --no-cache curl jq
    - |
      set -euo pipefail
      echo "=== Container Registry Cleanup ==="
      echo "Keeping last $KEEP_TAGS tags per repository"

      # Protected tags that should never be deleted
      PROTECTED_TAGS="latest main develop stable"

      # Helper: API call with retry
      api_call() {
        local url="$1"
        local method="${2:-GET}"
        for attempt in 1 2 3; do
          response=$(curl -s -w "\n%{http_code}" --request "$method" \
            --header "JOB-TOKEN: $CI_JOB_TOKEN" "$url" 2>/dev/null) || true
          http_code=$(echo "$response" | tail -1)
          body=$(echo "$response" | sed '$d')

          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            echo "$body"
            return 0
          fi
          echo "API call failed (attempt $attempt/3, HTTP $http_code)" >&2
          sleep 2
        done
        return 1
      }

      # Helper: Check if tag is protected
      is_protected() {
        local tag="$1"
        for protected in $PROTECTED_TAGS; do
          [ "$tag" = "$protected" ] && return 0
        done
        return 1
      }

      # Get all repositories
      REPOS=$(api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories" | \
        jq -r '.[] | "\(.id):\(.name)"' 2>/dev/null) || REPOS=""

      if [ -z "$REPOS" ]; then
        echo "No container repositories found or API error. Skipping cleanup."
        exit 0
      fi

      echo "Found repositories: $(echo "$REPOS" | cut -d: -f2 | tr '\n' ' ')"

      for repo_entry in $REPOS; do
        REPO_ID=$(echo "$repo_entry" | cut -d: -f1)
        REPO_NAME=$(echo "$repo_entry" | cut -d: -f2)
        echo ""
        echo "--- Processing: $REPO_NAME (ID: $REPO_ID) ---"

        # Fetch all tags with pagination (up to 200 tags)
        ALL_TAGS=""
        for page in 1 2; do
          PAGE_TAGS=$(api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$REPO_ID/tags?per_page=100&page=$page") || continue
          [ -z "$PAGE_TAGS" ] || [ "$PAGE_TAGS" = "[]" ] && break
          ALL_TAGS="$ALL_TAGS$PAGE_TAGS"
        done

        if [ -z "$ALL_TAGS" ] || [ "$ALL_TAGS" = "[]" ]; then
          echo "  No tags found, skipping."
          continue
        fi

        # Combine pages, sort by created_at (newest first), extract names
        SORTED_TAGS=$(echo "$ALL_TAGS" | jq -s 'add | sort_by(.created_at) | reverse | .[].name' -r 2>/dev/null) || continue
        TOTAL_TAGS=$(echo "$SORTED_TAGS" | wc -l)
        echo "  Found $TOTAL_TAGS tags"

        # Skip first N (keep them), delete the rest
        TAGS_TO_DELETE=$(echo "$SORTED_TAGS" | tail -n +$((KEEP_TAGS + 1)))
        DELETE_COUNT=$(echo "$TAGS_TO_DELETE" | grep -c . || echo 0)

        if [ "$DELETE_COUNT" -eq 0 ]; then
          echo "  Nothing to delete (only $TOTAL_TAGS tags, keeping $KEEP_TAGS)"
          continue
        fi

        echo "  Deleting $DELETE_COUNT old tags..."
        DELETED=0
        SKIPPED=0

        echo "$TAGS_TO_DELETE" | while read -r tag; do
          [ -z "$tag" ] && continue

          # Skip protected tags
          if is_protected "$tag"; then
            echo "    â­ Skipping protected: $tag"
            continue
          fi

          # Delete tag
          if api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$REPO_ID/tags/$tag" "DELETE" >/dev/null 2>&1; then
            echo "    âœ“ Deleted: $tag"
          else
            echo "    âœ— Failed: $tag"
          fi
        done

        echo "  Done with $REPO_NAME"
      done

      echo ""
      echo "=== Cleanup complete! ==="
  allow_failure: true

# ============ ROLLBACK (on failure) ============
rollback_prod:
  retry: 1
  timeout: 10m
  stage: verify
  image: alpine:3.20
  needs:
    - job: verify_prod
      optional: true
  rules:
    # Only available on release commits (same as deploy_prod)
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE =~ /^chore\(release\):/ && $CI_COMMIT_MESSAGE !~ /\[skip ci\]/'
      when: on_failure
    - when: never
  extends: .ssh_setup
  script:
    - |
      echo "Rolling back to previous version..."
      ssh $SSH_OPTS "$SSH_HOST" bash -s <<'ROLLBACK_SCRIPT'
        cd /opt/subro_web
        chmod +x ./infra/scripts/*.sh
        if [ -f ./infra/scripts/rollback.sh ]; then
          ./infra/scripts/rollback.sh
        else
          echo 'Rollback script not found, manual intervention required'
          exit 1
        fi
      ROLLBACK_SCRIPT
  environment:
    name: production
    action: stop
  allow_failure: true
