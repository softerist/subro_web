# ============================================================
# subro_web GitLab CI/CD Pipeline
# Based on project patterns, adapted for blue-green deploy
# ============================================================

stages:
  - pre
  - build
  - test
  - scan
  - staging
  - e2e
  - deploy
  - verify
  - release
  - notify
  - cleanup

# -------- Global Variables --------
variables:
  DOCKER_IMAGE_API: $CI_REGISTRY_IMAGE/api:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_WORKER: $CI_REGISTRY_IMAGE/worker:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_FRONTEND: $CI_REGISTRY_IMAGE/frontend:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_BACKUP: $CI_REGISTRY_IMAGE/backup:$CI_COMMIT_SHORT_SHA
  DOCKER_TLS_CERTDIR: ""

# -------- Docker-in-Docker Helper --------
.default-docker:
  image: docker:25
  services:
    - name: docker:25-dind
      command: ["--tls=false"]
  variables:
    DOCKER_HOST: tcp://docker:2375
  before_script:
    - set -euo pipefail
    - |
      for i in $(seq 1 3); do
        if echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"; then
          echo "Docker login to $CI_REGISTRY successful!"
          # Also login to Dependency Proxy server if it exists and is different
          if [ -n "${CI_DEPENDENCY_PROXY_SERVER:-}" ] && [ "$CI_DEPENDENCY_PROXY_SERVER" != "$CI_REGISTRY" ]; then
            echo "$CI_DEPENDENCY_PROXY_PASSWORD" | docker login -u "$CI_DEPENDENCY_PROXY_USER" --password-stdin "$CI_DEPENDENCY_PROXY_SERVER" || echo "Warning: Dependency Proxy login failed"
          fi
          break
        fi
        echo "Docker login failed (attempt $i/3). Retrying in 5 seconds..."
        sleep 5
      done

# -------- DEBUG (optional) --------
ci_debug_env:
  stage: pre
  image: alpine:3.20
  rules:
    - when: always
  allow_failure: true
  script:
    - 'echo "Pipeline: $CI_PIPELINE_SOURCE | Branch: $CI_COMMIT_BRANCH | SHA: $CI_COMMIT_SHORT_SHA"'

# ============ BUILD STAGE ============
.build_template:
  stage: build
  extends: .default-docker
  variables:
    DOCKER_BUILDKIT: "1"
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"
  rules:
    - if: "$CI_COMMIT_BRANCH"
  retry: 1

build_api:
  extends: .build_template
  variables:
    SERVICE: api
    DOCKER_IMAGE: $DOCKER_IMAGE_API
    TARGET: production-api
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend

build_worker:
  extends: .build_template
  variables:
    SERVICE: worker
    DOCKER_IMAGE: $DOCKER_IMAGE_WORKER
    TARGET: production-worker
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend

build_frontend:
  extends: .build_template
  variables:
    SERVICE: frontend
    DOCKER_IMAGE: $DOCKER_IMAGE_FRONTEND
    TARGET: production
    DOCKERFILE: frontend/Dockerfile
    CONTEXT: frontend

build_backup:
  extends: .build_template
  variables:
    SERVICE: backup
    DOCKER_IMAGE: $DOCKER_IMAGE_BACKUP
    TARGET: production
    DOCKERFILE: infra/docker/backup/Dockerfile
    CONTEXT: .
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --build-arg BASE_IMAGE="${CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX}/alpine:3.19" \
        --build-arg VITE_API_BASE_URL="${VITE_API_BASE_URL:-/api/v1}" \
        --build-arg VITE_WS_BASE_URL="${VITE_WS_BASE_URL:-/api/v1}" \
        --build-arg VITE_ONBOARDING_TOKEN="${VITE_ONBOARDING_TOKEN:-}" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

# ============ TEST STAGE ============
backend_tests:
  stage: test
  extends: .default-docker
  needs: [build_api]
  variables:
    # Database config for test container (hardcoded for CI)
    POSTGRES_USER: "admin"
    POSTGRES_PASSWORD: "ci_test_password_change_in_production"
    POSTGRES_DB: "subappdb"
    POSTGRES_DB_TEST: "subappdb_pytest"
  script:
    - apk add --no-cache bash
    - set -euo pipefail
    - |
      echo "Pulling required Docker images with retry..."
      DP_PREFIX="${CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX}/"
      [ -z "$CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX" ] && DP_PREFIX=""

      # Define images using Dependency Proxy where possible
      IMAGE_POSTGRES="${DP_PREFIX}postgres:16-alpine"
      IMAGE_REDIS="${DP_PREFIX}redis:7-alpine"
      IMAGE_CURL="${DP_PREFIX}curlimages/curl:8.7.1"

      DEPENDENCIES="$DOCKER_IMAGE_API $IMAGE_POSTGRES $IMAGE_REDIS $IMAGE_CURL"
      for img in $DEPENDENCIES; do
        echo "Pulling $img..."
        for i in $(seq 1 3); do
          if docker pull "$img"; then
            echo "Successfully pulled $img"
            # Tag back to standard name if it was pulled via proxy
            if [ -n "$DP_PREFIX" ] && [ "$img" != "$DOCKER_IMAGE_API" ]; then
              STANDARD_NAME=$(echo "$img" | sed "s|$DP_PREFIX||")
              docker tag "$img" "$STANDARD_NAME"
              echo "Tagged $img as $STANDARD_NAME"
            fi
            break
          fi
          if [ $i -eq 3 ]; then
            echo "ERROR: Failed to pull $img after 3 attempts."
            exit 1
          fi
          echo "Docker pull failed (attempt $i/3). Retrying in 5 seconds..."
          sleep 5
        done
      done
    - docker network create testnet || true
    # Start dependencies (Production-like setup)
    - docker run -d --name db --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB postgres:16-alpine
    - docker run --rm --network testnet --entrypoint python $DOCKER_IMAGE_API -c "import kombu, redis; print('DEBUG VERSIONS kombu', kombu.__version__, 'redis', redis.__version__)"
    - docker run -d --name db_test --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB_TEST postgres:16-alpine
    - docker run -d --name redis --network testnet redis:7-alpine

    # Wait for databases to be ready
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet postgres:16-alpine pg_isready -h db -U $POSTGRES_USER && \
           docker run --rm --network testnet postgres:16-alpine pg_isready -h db_test -U $POSTGRES_USER; then
          echo "Databases ready!"
          break
        fi
        echo "Waiting for Databases ($i/30)..." && sleep 2
      done

    # Start API with env file from repo, override network-specific vars
    - |
      docker run -d --name api --network testnet \
        --env-file backend/.env.test \
        -e DATABASE_URL="postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}" \
        -e DB_HOST="db" \
        -e POSTGRES_SERVER="db" \
        -e REDIS_HOST="redis" \
        -e CELERY_BROKER_URL="redis://redis:6379/0" \
        -e CELERY_RESULT_BACKEND="redis://redis:6379/1" \
        -e REDIS_PUBSUB_URL="redis://redis:6379/2" \
        -e TEST_DATABASE_HOST="db_test" \
        -e TEST_DATABASE_PORT="5432" \
        -e TEST_API_BASE_URL="http://localhost:8000" \
        -e TEST_WS_BASE_URL="ws://localhost:8000" \
        -v "$(pwd)/backend/tests:/app/tests" \
        "$DOCKER_IMAGE_API"

    # Wait for API to be healthy
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet curlimages/curl:8.7.1 -fsS http://api:8000/health 2>/dev/null; then
          echo "API healthy!"
          break
        fi
        echo "Waiting for API ($i/30)..." && sleep 2
      done

    # Install test dependencies and run tests
    - docker exec api poetry install --with dev
    - docker exec api poetry run pytest -p no:cacheprovider tests/
  after_script:
    - docker logs api 2>&1 | tail -50 || true
    - docker rm -f api db db_test redis || true

frontend_tests:
  stage: test
  image: node:20-alpine
  needs: []
  variables:
    NPM_CONFIG_CACHE: "$CI_PROJECT_DIR/frontend/.npm"
    VITE_API_BASE_URL: "${VITE_API_BASE_URL:-http://localhost:8000/api/v1}"
    VITE_WS_BASE_URL: "${VITE_WS_BASE_URL:-ws://localhost:8000/api/v1}"
    VITE_ONBOARDING_TOKEN: "${VITE_ONBOARDING_TOKEN}"
    VITE_API_PROXY_TARGET: "${VITE_API_PROXY_TARGET:-http://localhost:8000}"
  cache:
    key:
      files:
        - frontend/package-lock.json
    policy: pull-push
    paths:
      - frontend/node_modules/
      - frontend/.npm/
  before_script:
    - set -euo pipefail
    - cd frontend
    - npm config set cache .npm
  script:
    - npm ci --prefer-offline --no-audit --no-fund
    - npm test

# ============ SCAN STAGE ============
scan_image_vulns:
  stage: scan
  image:
    name: aquasec/trivy:0.53.0
    entrypoint: [""]
  needs: [build_api, build_worker, build_frontend]
  variables:
    TRIVY_SEVERITY: "HIGH,CRITICAL"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        TRIVY_SEVERITY: "CRITICAL"
    - when: on_success
  cache:
    key: trivy-cache
    paths: [.trivycache/]
  script:
    - |
      set -euo pipefail
      export TRIVY_USERNAME="$CI_REGISTRY_USER"
      export TRIVY_PASSWORD="$CI_REGISTRY_PASSWORD"
      for img in "$DOCKER_IMAGE_API" "$DOCKER_IMAGE_FRONTEND" "$DOCKER_IMAGE_WORKER"; do
        echo "Scanning $img..."
        trivy image --cache-dir .trivycache --exit-code 1 \
          --severity "$TRIVY_SEVERITY" --timeout 10m --no-progress \
          --skip-files "/app/secrets/google-api-config.json" \
          "$img"
      done
  allow_failure: false
  retry: 1

sast_scan:
  stage: scan
  image:
    name: returntocorp/semgrep:latest
    entrypoint: [""]
  variables:
    SAST_MIN_SEVERITY: "WARNING"
    SAST_FAIL_ON_FINDINGS: "false"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        SAST_MIN_SEVERITY: "ERROR"
        SAST_FAIL_ON_FINDINGS: "true"
    - when: on_success
  script:
    - |
      # Debug: Check if token is set (without revealing value)
      if [ -z "${SEMGREP_APP_TOKEN:-}" ]; then
        echo "ERROR: SEMGREP_APP_TOKEN is not set or empty."
        echo "Please configure it in GitLab CI/CD Variables."
        exit 2
      fi
      echo "‚úì SEMGREP_APP_TOKEN is set (length: ${#SEMGREP_APP_TOKEN})"

      # Use 'semgrep ci' for authenticated CI runs (automatically uses SEMGREP_APP_TOKEN)
      # This is the recommended way to run Semgrep in CI with cloud features
      semgrep ci --sarif -o semgrep.sarif
  artifacts:
    reports:
      sast: semgrep.sarif
    when: always
  allow_failure: false

secret_scan:
  stage: scan
  image:
    name: aquasec/trivy:0.53.0
    entrypoint: [""]
  script:
    - |
      trivy fs --scanners secret \
        --skip-dirs ".git,__pycache__,.venv,venv,node_modules,.ruff_cache,.pytest_cache" \
        --exit-code 1 --no-progress --timeout 5m .
  allow_failure: false

# ============ STAGING STAGE ============
deploy_staging:
  stage: staging
  image: alpine:3.20
  resource_group: staging
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  needs:
    - job: backend_tests
    - job: scan_image_vulns
    - job: sast_scan
    - job: secret_scan
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  script:
    - |
      set -euo pipefail
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        set -euo pipefail

        STAGING_DEPLOY_DIR=\"\${STAGING_DEPLOY_DIR:-/opt/subro_web_staging}\"
        echo \"Using staging deploy dir: \$STAGING_DEPLOY_DIR\"

        # Ensure staging directory exists
        mkdir -p \"\$STAGING_DEPLOY_DIR\"
        cd \"\$STAGING_DEPLOY_DIR\"

        # Clone or update repo
        if [ ! -d .git ]; then
          git clone --depth 1 git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git .
        else
          git fetch --depth 1 origin $CI_COMMIT_SHA
          git checkout -f $CI_COMMIT_SHA
          git clean -fd
        fi
        echo \"Checked out commit: \$(git rev-parse HEAD)\"

        # Setup staging environment
        mkdir -p infra
        echo '$PROD_ENV_FILE' | base64 -d > infra/.env.staging
        chmod 600 infra/.env.staging

        # Increase Docker/Compose timeouts for registry operations
        export DOCKER_CLIENT_TIMEOUT="${DOCKER_CLIENT_TIMEOUT:-300}"
        export COMPOSE_HTTP_TIMEOUT="${COMPOSE_HTTP_TIMEOUT:-300}"

        docker_login_with_retry() {
          local max_attempts=3
          local attempt=1
          local wait_time=5

          echo "Logging into Docker Registry with retry (max $max_attempts attempts)..."
          while [ $attempt -le $max_attempts ]; do
            if echo '$REGISTRY_PASSWORD' | docker login -u '$REGISTRY_USER' --password-stdin $CI_REGISTRY; then
              echo "Docker login successful!"
              return 0
            fi
            if [ $attempt -eq $max_attempts ]; then
              echo "ERROR: Docker login failed after $max_attempts attempts."
              return 1
            fi
            echo "Docker login failed (attempt $attempt/$max_attempts). Retrying in ${wait_time}s..."
            sleep $wait_time
            wait_time=$((wait_time * 2))
            attempt=$((attempt + 1))
          done
        }

        docker_login_with_retry

        # Set image tags for remote environment
        export DOCKER_IMAGE_API='$DOCKER_IMAGE_API'
        export DOCKER_IMAGE_WORKER='$DOCKER_IMAGE_WORKER'
        export DOCKER_IMAGE_FRONTEND='$DOCKER_IMAGE_FRONTEND'
        export DOCKER_IMAGE_BACKUP='$DOCKER_IMAGE_BACKUP'
        export BACKUP_PREFIX=staging_
        export USE_PREBUILT_IMAGES=1
        export DEPLOY_ENV=staging

        # Deploy staging
        chmod +x ./infra/scripts/*.sh
        ./infra/scripts/deploy_staging.sh

        docker logout $CI_REGISTRY
      "
  environment:
    name: staging
    url: https://staging.$SSH_HOST/
  retry: 1

verify_staging:
  stage: staging
  image: alpine:3.20
  needs: [deploy_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  script:
    - apk add --no-cache curl
    - |
      # Resolve SSH_HOST to IP and add staging subdomain to /etc/hosts
      SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1)
      if [ -z "$SERVER_IP" ]; then
        echo "ERROR: Could not resolve IP for $SSH_HOST"
        exit 1
      fi
      echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
      echo "Added /etc/hosts entry: $SERVER_IP staging.$SSH_HOST"
    - |
      echo "Verifying staging deployment..."
      echo ""
      echo "=== DEBUG: Testing different endpoints ==="

      # Test 1: /api/v1/ with verbose output
      echo "--- Test: /api/v1/ ---"
      curl -k -v --max-time 10 "https://staging.$SSH_HOST/api/v1/" 2>&1 | head -50 || true
      echo ""

      # Test 2: Try a simple curl to see HTTP response
      echo "--- Test: Simple response check ---"
      HTTP_CODE=$(curl -k -s -o /dev/null -w "%{http_code}" --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "000")
      RESPONSE=$(curl -k -s --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "")
      echo "HTTP Code: $HTTP_CODE"
      echo "Response length: ${#RESPONSE}"
      echo "Response: >>$RESPONSE<<"
      echo ""

      # Check if API is responding with valid content
      if echo "$RESPONSE" | grep -qE '"(version|message)"'; then
        echo "‚úÖ Staging API is responding correctly!"
        exit 0
      fi

      echo ""
      echo "‚ö†Ô∏è  WARNING: External Nginx is intercepting requests and returning empty 200."
      echo "    The staging containers may be running fine, but requests don't reach them."
      echo "    Please check your external Nginx config for staging.secure.go.ro"
      echo ""
      echo "    To fix: Update Nginx to proxy_pass to Caddy (port 8080)"
      exit 1
  environment:
    name: staging
    url: https://staging.$SSH_HOST/

# ============ E2E TESTS STAGE ============
e2e_tests:
  stage: e2e
  image: mcr.microsoft.com/playwright:v1.40.0-jammy
  needs: [verify_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    E2E_BASE_URL: https://staging.$SSH_HOST
  script:
    - SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1) && echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
    - cd frontend
    - npm ci --prefer-offline
    - npx playwright install --with-deps chromium
    - npx playwright test --reporter=html
  artifacts:
    when: always
    paths:
      - frontend/playwright-report/
    expire_in: 7 days

# Cleanup staging after E2E tests (ephemeral staging)
cleanup_staging:
  stage: e2e
  image: alpine:3.20
  needs:
    - job: e2e_tests
      artifacts: false
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: always # Run even if e2e_tests fail
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  script:
    - |
      echo "Stopping staging environment..."
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        set -euo pipefail
        STAGING_DIR=\"\${STAGING_DEPLOY_DIR:-/opt/subro_web_staging}\"

        if [ -d \"\$STAGING_DIR\" ]; then
          cd \"\$STAGING_DIR\"

          # Stop staging containers (app stack)
          docker compose -p subro_staging \
            --env-file infra/.env.staging \
            -f infra/docker/compose.prod.yml \
            down --remove-orphans 2>/dev/null || true

          echo '‚úì Staging environment stopped'
        else
          echo 'Staging directory not found, nothing to clean up'
        fi
      "
  allow_failure: true # Don't block pipeline if cleanup fails

# ============ DEPLOY STAGE ============
.deploy_template:
  stage: deploy
  image: alpine:3.20
  resource_group: $CI_ENVIRONMENT_NAME # Serializes deploys per environment
  needs:
    - job: backend_tests
    - job: scan_image_vulns
    - job: sast_scan
    - job: secret_scan
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  retry: 1

deploy_prod:
  extends: .deploy_template
  needs:
    - job: e2e_tests
      artifacts: false
  environment:
    name: production
    url: https://$SSH_HOST/
    deployment_tier: production
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
    - when: never
  script:
    - |
      set -euo pipefail
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        set -euo pipefail

        # Ensure deploy directory exists (first-time setup)
        mkdir -p /opt/subro_web
        cd /opt/subro_web

        # Clone repo if first deploy (infra scripts needed)
        if [ ! -d .git ]; then
          git clone --depth 1 git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git .
        else
          git fetch --depth 1 origin $CI_COMMIT_SHA
          git checkout -f $CI_COMMIT_SHA
          git clean -fd
        fi

        # Ensure infra directory exists and decode .env.prod
        mkdir -p infra
        echo '$PROD_ENV_FILE' | base64 -d > infra/.env.prod
        chmod 600 infra/.env.prod

        # Increase Docker/Compose timeouts for registry operations
        export DOCKER_CLIENT_TIMEOUT="${DOCKER_CLIENT_TIMEOUT:-300}"
        export COMPOSE_HTTP_TIMEOUT="${COMPOSE_HTTP_TIMEOUT:-300}"

        docker_login_with_retry() {
          local max_attempts=3
          local attempt=1
          local wait_time=5

          echo "Logging into Docker Registry with retry (max $max_attempts attempts)..."
          while [ $attempt -le $max_attempts ]; do
            if echo '$REGISTRY_PASSWORD' | docker login -u '$REGISTRY_USER' --password-stdin $CI_REGISTRY; then
              echo "Docker login successful!"
              return 0
            fi
            if [ $attempt -eq $max_attempts ]; then
              echo "ERROR: Docker login failed after $max_attempts attempts."
              return 1
            fi
            echo "Docker login failed (attempt $attempt/$max_attempts). Retrying in ${wait_time}s..."
            sleep $wait_time
            wait_time=$((wait_time * 2))
            attempt=$((attempt + 1))
          done
        }

        docker_login_with_retry

        # Set image tags for remote environment
        export DOCKER_IMAGE_API='$DOCKER_IMAGE_API'
        export DOCKER_IMAGE_WORKER='$DOCKER_IMAGE_WORKER'
        export DOCKER_IMAGE_FRONTEND='$DOCKER_IMAGE_FRONTEND'
        export DOCKER_IMAGE_BACKUP='$DOCKER_IMAGE_BACKUP'
        export USE_PREBUILT_IMAGES=1

        # Ensure scripts are executable
        chmod +x ./infra/scripts/*.sh

        # Deploy
        ./infra/scripts/blue_green_deploy.sh

        # Cleanup
        docker logout $CI_REGISTRY
      "

# ============ VERIFY STAGE ============
verify_prod:
  stage: verify
  image: alpine:3.20
  needs: [deploy_prod]
  environment:
    name: production
    url: https://$SSH_HOST/
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl
    - |
      echo "Verifying production deployment..."
      for i in $(seq 1 10); do
        RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/health" 2>&1 || true)
        if ! echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/api/v1/healthz" 2>&1 || true)
        fi
        if echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          echo "Production is healthy! Response: $RESPONSE"
          exit 0
        fi
        echo "Waiting for production ($i/10)... Response: $RESPONSE"
        sleep 5
      done
      echo "Production health check failed!"
      exit 1

# ============ RELEASE STAGE ============
release_version:
  stage: release
  image: alpine:3.20
  variables:
    GIT_STRATEGY: clone
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE !~ /chore\(release\): bump version/'
      when: on_success
  environment:
    name: production
    action: access
  before_script:
    - set -euo pipefail
    - apk add --no-cache git openssh-client openssh-keygen bash python3 py3-pip
    - pip install --break-system-packages tomlkit
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan "$CI_SERVER_HOST" >> ~/.ssh/known_hosts
    - git config --global user.email "ci-bot@gitlab.com"
    - git config --global user.name "CI Bot"
    - git remote set-url origin git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git
  script:
    - echo "Bumping version..."
    - python3 backend/scripts/bump_version.py

    # Check for changes
    - |
      if [[ -n $(git status --porcelain) ]]; then
        git add backend/pyproject.toml backend/app/core/config.py
        # Extract new version for commit message
        NEW_VERSION=$(grep 'version = ' backend/pyproject.toml | head -n 1 | cut -d '"' -f 2)
        git commit -m "chore(release): bump version to $NEW_VERSION"
        git push origin HEAD:$CI_COMMIT_BRANCH
        git push gitlab HEAD:$CI_COMMIT_BRANCH
        echo "Version bumped to $NEW_VERSION and pushed to origin and gitlab ($CI_COMMIT_BRANCH)"
      else
        echo "No version changes detected."
      fi

# ============ NOTIFY STAGE ============
notify_success:
  stage: notify
  image: alpine:3.20
  needs:
    - job: verify_prod
    - job: release_version
      optional: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $SLACK_WEBHOOK_URL'
      when: on_success
  script:
    - apk add --no-cache curl
    - |
      VERSION=$(grep 'version = ' backend/pyproject.toml | head -n 1 | cut -d '"' -f 2 || echo "unknown")
      curl -X POST "$SLACK_WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"‚úÖ *Deploy successful*\\nüì¶ $CI_PROJECT_NAME v$VERSION\\nüîó <$CI_PIPELINE_URL|Pipeline #$CI_PIPELINE_ID>\"}"
  allow_failure: true

notify_failure:
  stage: notify
  image: alpine:3.20
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $SLACK_WEBHOOK_URL'
      when: on_failure
  script:
    - apk add --no-cache curl
    - |
      curl -X POST "$SLACK_WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"‚ùå *Deploy FAILED*\\nüì¶ $CI_PROJECT_NAME\\nüîó <$CI_PIPELINE_URL|Pipeline #$CI_PIPELINE_ID>\"}"
  allow_failure: true

# ============ CLEANUP STAGE ============
cleanup_registry:
  stage: cleanup
  image: alpine:3.20
  needs:
    - job: notify_success
      optional: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl jq
    - |
      echo "Cleaning up old container images..."
      # Keep last 10 tags, delete older ones
      # This uses GitLab Container Registry API
      for repo in api worker frontend; do
        echo "Checking $repo repository..."
        TAGS=$(curl -s --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
          "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories" | \
          jq -r ".[] | select(.name==\"$repo\") | .id" || true)

        if [ -n "$TAGS" ]; then
          echo "Found repository ID: $TAGS for $repo"
          # List and delete old tags (keeping last 10)
          curl -s --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
            "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$TAGS/tags" | \
            jq -r '.[10:] | .[].name' | while read tag; do
              echo "Deleting old tag: $tag"
              curl -s --request DELETE --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
                "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$TAGS/tags/$tag" || true
            done
        fi
      done
      echo "Cleanup complete!"
  allow_failure: true

# ============ ROLLBACK (on failure) ============
rollback_prod:
  stage: verify
  image: alpine:3.20
  needs: [verify_prod]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_failure
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  script:
    - |
      echo "Rolling back to previous version..."
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        cd /opt/subro_web
        chmod +x ./infra/scripts/*.sh
        if [ -f ./infra/scripts/rollback.sh ]; then
          ./infra/scripts/rollback.sh
        else
          echo 'Rollback script not found, manual intervention required'
          exit 1
        fi
      "
  environment:
    name: production
    action: stop
  allow_failure: true
