# ============================================================
# subro_web GitLab CI/CD Pipeline
# Based on project patterns, adapted for blue-green deploy
# ============================================================

stages:
  - lint
  - pre
  - build
  - test
  - scan
  - staging
  - e2e
  - deploy
  - verify
  - release
  - cleanup

# -------- Templates --------
.ssh_setup:
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - |
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > ~/.ssh/id_rsa
      fi
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
    - |
      cat > ~/.ssh/config <<EOF
      Host *
        User $SSH_USER
        Port ${SSH_PORT:-22}
        ServerAliveInterval 15
        IdentityFile ~/.ssh/id_rsa
      EOF
    - chmod 600 ~/.ssh/config

# ============ LINT STAGE ============
lint_backend:
  stage: lint
  image: python:3.12-slim
  before_script:
    - export POETRY_VERSION=$(awk '/poetry/ {print $2}' .tool-versions)
    - pip install poetry==$POETRY_VERSION
    - cd backend
    - poetry install --no-root --only dev
  script:
    - poetry run ruff check .
    - poetry run ruff format --check .
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"

lint_frontend:
  stage: lint
  image: node:20-alpine
  before_script:
    - cd frontend
    - npm ci --prefer-offline
  script:
    - npm run lint
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_BRANCH == "develop"
    - if: $CI_COMMIT_TAG

# -------- Global Variables --------
variables:
  DOCKER_IMAGE_API: $CI_REGISTRY_IMAGE/api:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_API_TEST: $CI_REGISTRY_IMAGE/api-test:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_WORKER: $CI_REGISTRY_IMAGE/worker:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_FRONTEND: $CI_REGISTRY_IMAGE/frontend:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_BACKUP: $CI_REGISTRY_IMAGE/backup:$CI_COMMIT_SHORT_SHA
  DOCKER_TLS_CERTDIR: ""
  GIT_STRATEGY: fetch
  GIT_DEPTH: "20"
  GIT_SUBMODULE_STRATEGY: none

# -------- Docker-in-Docker Helper --------
.default-docker:
  image: docker:25
  services:
    - name: docker:25-dind
      command: ["--tls=false"]
  variables:
    DOCKER_HOST: tcp://docker:2375
  before_script:
    - set -euo pipefail
    - |
      for i in $(seq 1 3); do
        if echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"; then
          echo "Docker login successful!"
          if [ -n "${CI_DEPENDENCY_PROXY_SERVER:-}" ] && [ "$CI_DEPENDENCY_PROXY_SERVER" != "$CI_REGISTRY" ]; then
            echo "$CI_DEPENDENCY_PROXY_PASSWORD" | docker login -u "$CI_DEPENDENCY_PROXY_USER" --password-stdin "$CI_DEPENDENCY_PROXY_SERVER" || echo "Warning: Dependency Proxy login failed"
          fi
          break
        fi
        echo "Docker login failed (attempt $i/3). Retrying in 5 seconds..."
        sleep 5
      done
    - |
      if [ -n "${DOCKERHUB_USER:-}" ] && [ -n "${DOCKERHUB_TOKEN:-}" ]; then
        for i in $(seq 1 3); do
          if echo "$DOCKERHUB_TOKEN" | docker login -u "$DOCKERHUB_USER" --password-stdin; then
            echo "Docker Hub login successful!"
            break
          fi
          echo "Docker Hub login failed (attempt $i/3). Retrying in 5 seconds..."
          sleep 5
        done
      else
        echo "Docker Hub credentials not set; skipping Docker Hub login."
      fi

# -------- DEBUG (optional) --------
ci_debug_env:
  stage: pre
  image: alpine:3.20
  rules:
    - when: always
  allow_failure: true
  script:
    - 'echo "Pipeline: $CI_PIPELINE_SOURCE | Branch: $CI_COMMIT_BRANCH | SHA: $CI_COMMIT_SHORT_SHA"'

# ============ BUILD STAGE ============
.build_template:
  stage: build
  extends: .default-docker
  variables:
    DOCKER_BUILDKIT: "1"
  script:
    - |
      set -euo pipefail
      export POETRY_VERSION=$(awk '/poetry/ {print $2}' .tool-versions)
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        --build-arg POETRY_VERSION="$POETRY_VERSION" \
        "$CONTEXT"
  rules:
    - when: on_success
  retry: 1

build_api:
  extends: .build_template
  variables:
    SERVICE: api
    DOCKER_IMAGE: $DOCKER_IMAGE_API
    TARGET: production-api
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --build-arg BASE_IMAGE="public.ecr.aws/docker/library/ubuntu:24.04" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

build_api_test:
  extends: .build_template
  variables:
    SERVICE: api-test
    DOCKER_IMAGE: $DOCKER_IMAGE_API_TEST
    TARGET: ci-test
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --build-arg BASE_IMAGE="public.ecr.aws/docker/library/ubuntu:24.04" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

build_worker:
  extends: .build_template
  variables:
    SERVICE: worker
    DOCKER_IMAGE: $DOCKER_IMAGE_WORKER
    TARGET: production-worker
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --build-arg BASE_IMAGE="public.ecr.aws/docker/library/ubuntu:24.04" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

build_frontend:
  extends: .build_template
  variables:
    SERVICE: frontend
    DOCKER_IMAGE: $DOCKER_IMAGE_FRONTEND
    TARGET: production
    DOCKERFILE: frontend/Dockerfile
    CONTEXT: frontend
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --build-arg BASE_IMAGE_NODE="public.ecr.aws/docker/library/node:20-alpine" \
        --build-arg BASE_IMAGE_NGINX="public.ecr.aws/docker/library/nginx:stable-alpine" \
        --build-arg VITE_API_BASE_URL="${VITE_API_BASE_URL:-/api/v1}" \
        --build-arg VITE_WS_BASE_URL="${VITE_WS_BASE_URL:-/api/v1}" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

build_backup:
  extends: .build_template
  variables:
    SERVICE: backup
    DOCKER_IMAGE: $DOCKER_IMAGE_BACKUP
    TARGET: production
    DOCKERFILE: infra/docker/backup/Dockerfile
    CONTEXT: .
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$DOCKER_IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --provenance=false \
        --sbom=false \
        --build-arg BASE_IMAGE="public.ecr.aws/docker/library/alpine:3.19" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

# ============ TEST STAGE ============
backend_tests:
  stage: test
  extends: .default-docker
  needs: [build_api_test]
  timeout: 20m
  interruptible: false # Prevent cancellation during test/cleanup
  variables:
    # Database config for test container - use CI/CD variables with fallback defaults
    POSTGRES_USER: "${CI_TEST_POSTGRES_USER}"
    POSTGRES_PASSWORD: "${CI_TEST_POSTGRES_PASSWORD}"
    POSTGRES_DB: "${CI_TEST_POSTGRES_DB}"
    POSTGRES_DB_TEST: "${CI_TEST_POSTGRES_DB_TEST}"
  script:
    - apk add --no-cache bash
    - |
      set -euo pipefail
      # Define images using AWS ECR Public Gallery (reliable mirror)
      IMAGE_POSTGRES="public.ecr.aws/docker/library/postgres:16-alpine"
      IMAGE_REDIS="public.ecr.aws/docker/library/redis:7-alpine"
      # Use alpine with curl to avoid Docker Hub rate limits
      IMAGE_CURL="public.ecr.aws/docker/library/alpine:3.20"

      DEPENDENCIES="$DOCKER_IMAGE_API_TEST $IMAGE_POSTGRES $IMAGE_REDIS $IMAGE_CURL"
      for img in $DEPENDENCIES; do
        echo "Pulling $img..."
        for i in $(seq 1 3); do
          if docker pull "$img"; then
            echo "Successfully pulled $img"
            # Tag back to standard name if it was pulled from mirror
            if [[ "$img" == *"public.ecr.aws"* ]]; then
               # Extract base name e.g. public.ecr.aws/docker/library/postgres:16-alpine -> postgres:16-alpine
               STANDARD_NAME=$(echo "$img" | sed 's|public.ecr.aws/docker/library/||')
               docker tag "$img" "$STANDARD_NAME"
               echo "Tagged $img as $STANDARD_NAME"
            fi
            break
          fi
          if [ $i -eq 3 ]; then
            echo "ERROR: Failed to pull $img after 3 attempts."
            exit 1
          fi
          echo "Docker pull failed (attempt $i/3). Retrying in 5 seconds..."
          sleep 5
        done
      done
    - docker network create testnet || true
    # Start dependencies (Production-like setup)
    - docker run -d --name db --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB postgres:16-alpine
    - docker run --rm --network testnet --entrypoint python $DOCKER_IMAGE_API_TEST -c "import kombu, redis; print('DEBUG VERSIONS kombu', kombu.__version__, 'redis', redis.__version__)"
    - docker run -d --name db_test --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB_TEST postgres:16-alpine
    - docker run -d --name redis --network testnet redis:7-alpine

    # Wait for databases to be ready
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet postgres:16-alpine pg_isready -h db -U $POSTGRES_USER && \
           docker run --rm --network testnet postgres:16-alpine pg_isready -h db_test -U $POSTGRES_USER; then
          echo "Databases ready!"
          break
        fi
        echo "Waiting for Databases ($i/30)..." && sleep 2
      done

    # Start API with env file from repo, override network-specific vars
    - |
      docker run -d --name api --network testnet \
        --env-file backend/.env.test \
        -e DATABASE_URL="postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}" \
        -e DB_HOST="db" \
        -e POSTGRES_SERVER="db" \
        -e REDIS_HOST="redis" \
        -e CELERY_BROKER_URL="redis://redis:6379/0" \
        -e CELERY_RESULT_BACKEND="redis://redis:6379/1" \
        -e REDIS_PUBSUB_URL="redis://redis:6379/2" \
        -e TEST_DATABASE_HOST="db_test" \
        -e TEST_DATABASE_PORT="5432" \
        -e TEST_API_BASE_URL="http://localhost:8000" \
        -e TEST_WS_BASE_URL="ws://localhost:8000" \
        -v "$(pwd)/backend/tests:/app/tests" \
        "$DOCKER_IMAGE_API_TEST"

    # Wait for API to be healthy
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet alpine:3.20 sh -c 'apk add --no-cache -q curl && curl -fsS http://api:8000/health' 2>/dev/null; then
          echo "API healthy!"
          break
        fi
        echo "Waiting for API ($i/30)..." && sleep 2
      done

    # Install test dependencies and run tests
    - docker exec api poetry run pytest -p no:cacheprovider tests/
  after_script:
    - docker logs api 2>&1 | tail -50 || true
    # Forceful cleanup - ensure containers are removed even on abort
    - docker rm -f api db db_test redis 2>/dev/null || true
    - docker network rm testnet 2>/dev/null || true

frontend_tests:
  stage: test
  image: node:20-alpine
  needs: []
  timeout: 10m
  variables:
    NPM_CONFIG_CACHE: "$CI_PROJECT_DIR/frontend/.npm"
    VITE_API_BASE_URL: "${VITE_API_BASE_URL:-http://localhost:8000/api/v1}"
    VITE_WS_BASE_URL: "${VITE_WS_BASE_URL:-ws://localhost:8000/api/v1}"
    VITE_API_PROXY_TARGET: "${VITE_API_PROXY_TARGET:-http://localhost:8000}"
  cache:
    key:
      files:
        - frontend/package-lock.json
    policy: pull-push
    paths:
      - frontend/node_modules/
      - frontend/.npm/
  before_script:
    - set -euo pipefail
    - cd frontend
    - npm config set cache .npm
  script:
    - npm ci --prefer-offline --no-audit --no-fund
    - npm test -- --maxWorkers=4 --coverage=false

# ============ SCAN STAGE ============
scan_image_vulns:
  stage: scan
  image:
    name: aquasec/trivy:0.58.0 # Updated from 0.53.0
    entrypoint: [""]
  needs: [build_api, build_worker, build_frontend, build_backup]
  timeout: 15m
  variables:
    TRIVY_SEVERITY: "HIGH,CRITICAL"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        TRIVY_SEVERITY: "CRITICAL"
    - when: on_success
  cache:
    key: trivy-cache-v2
    paths: [.trivycache/]
  script:
    - |
      set -euo pipefail
      export TRIVY_USERNAME="$CI_REGISTRY_USER"
      export TRIVY_PASSWORD="$CI_REGISTRY_PASSWORD"

      # Scan images (trivy is fast enough that sequential is fine)
      for img in "$DOCKER_IMAGE_API" "$DOCKER_IMAGE_FRONTEND" "$DOCKER_IMAGE_WORKER" "$DOCKER_IMAGE_BACKUP"; do
        echo "Scanning $img..."
        # Retry scan up to 3 times to handle eventual consistency or network flakes
        for attempt in 1 2 3; do
          if trivy image --cache-dir .trivycache --exit-code 1 \
            --severity "$TRIVY_SEVERITY" --timeout 5m --no-progress \
            --skip-files "/app/secrets/google-api-config.json" \
            --scanners vuln \
            "$img"; then
            echo "✓ Scan successful for $img"
            break
          fi
          if [ $attempt -eq 3 ]; then
            echo "ERROR: Failed to scan $img after 3 attempts"
            exit 1
          fi
          echo "Scan failed (attempt $attempt/3). Retrying in 10s..."
          sleep 10
        done
      done
      echo "All image scans passed!"
  allow_failure: false
  retry: 1

sast_scan:
  stage: scan
  image:
    name: returntocorp/semgrep:1.90.0
    entrypoint: [""]
  timeout: 10m
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - when: on_success
  script:
    - |
      echo "Running Semgrep SAST scan..."

      # Use 'semgrep scan' with specific configs (auto requires metrics)
      # p/default: general security rules
      # p/python: Python-specific rules
      # p/typescript: TypeScript/JavaScript rules
      semgrep scan \
        --config p/default \
        --config p/python \
        --config p/typescript \
        --sarif -o semgrep.sarif \
        --error \
        --exclude="*.lock" \
        --exclude="**/package-lock.json" \
        --exclude="**/poetry.lock" \
        --exclude="**/node_modules" \
        --exclude="**/.venv" \
        --exclude="**/dist" \
        --exclude="**/build" \
        --exclude="**/alembic/versions" \
        --jobs 4 \
        --max-target-bytes 500000 \
        --timeout 120 \
        .

      echo "✓ Semgrep scan complete - no findings"
  artifacts:
    reports:
      sast: semgrep.sarif
    expire_in: 2 days
    when: always
  allow_failure: false
  retry: 1

secret_scan:
  stage: scan
  image:
    name: aquasec/trivy:0.58.0
    entrypoint: [""]
  script:
    - |
      trivy fs --scanners secret \
        --skip-dirs ".git,__pycache__,.venv,venv,node_modules,.ruff_cache,.pytest_cache" \
        --exit-code 1 --no-progress --timeout 5m .
  allow_failure: false
  retry: 1

# ============ STAGING STAGE ============
deploy_staging:
  stage: staging
  image: alpine:3.20
  resource_group: staging
  timeout: 15m
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  needs:
    - job: backend_tests
      artifacts: false
    - job: frontend_tests
      artifacts: false
    - job: build_api
      artifacts: false
    - job: build_worker
      artifacts: false
    - job: build_frontend
      artifacts: false
    - job: build_backup
      artifacts: false
    - job: scan_image_vulns
      artifacts: false
    - job: sast_scan
      artifacts: false
    - job: secret_scan
      artifacts: false
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  extends: .ssh_setup
  script:
    - |
      set -euo pipefail

      # 0. Ensure server-side SSH keepalives are configured (idempotent)
      ssh "$SSH_HOST" \
        'if [ ! -f /etc/ssh/sshd_config.d/keepalive.conf ]; then
          printf "%s\n" "# CI/CD keepalives" "ClientAliveInterval 15" "ClientAliveCountMax 10" "TCPKeepAlive yes" > /etc/ssh/sshd_config.d/keepalive.conf &&
          (systemctl reload ssh 2>/dev/null || systemctl reload sshd 2>/dev/null || true) &&
          echo "SSH keepalives configured";
        fi'

      # 1. Prepare Directory & Checkout
      ssh "$SSH_HOST" "
        set -euo pipefail
        mkdir -p \"${STAGING_DEPLOY_DIR}\"
        cd \"${STAGING_DEPLOY_DIR}\"
        if [ ! -d .git ]; then
          git clone --depth 1 \"git@${CI_SERVER_HOST}:${CI_PROJECT_PATH}.git\" .
        else
          git fetch --depth 1 origin \"${CI_COMMIT_SHA}\"
          git checkout -f \"${CI_COMMIT_SHA}\"
          git clean -fd
        fi
        CURRENT_COMMIT=\$(git rev-parse HEAD)
        echo \"Checked out commit: \$CURRENT_COMMIT\"
        if [ \"\$CURRENT_COMMIT\" != \"${CI_COMMIT_SHA}\" ]; then
          echo \"❌ ERROR: Checkout verification failed!\"
          exit 1
        fi
      "

      # 2. Verify Docker images exist before deploying
      echo ""
      echo "=== Verifying Docker Images ==="
      ssh "$SSH_HOST" "
        set -euo pipefail

        # Docker login with retry (handles registry timeouts)
        for i in 1 2 3; do
          if echo '${REGISTRY_PASSWORD}' | docker login -u '${REGISTRY_USER}' --password-stdin '${CI_REGISTRY}' 2>/dev/null; then
            echo 'Docker login successful'
            break
          fi
          if [ \$i -eq 3 ]; then
            echo '❌ Docker login failed after 3 attempts'
            exit 1
          fi
          echo \"Docker login attempt \$i failed, retrying in 5s...\"
          sleep 5
        done
        echo 'Checking images...'
        # Wait for registry to propagate newly pushed images
        echo 'Waiting 10 seconds for registry propagation...'
        sleep 10

        for img in '${DOCKER_IMAGE_API}' '${DOCKER_IMAGE_WORKER}' '${DOCKER_IMAGE_FRONTEND}' '${DOCKER_IMAGE_BACKUP}'; do
          echo \"Verifying: \$img\"
          for attempt in 1 2 3; do
            if docker manifest inspect \"\$img\" > /dev/null 2>&1; then
              echo \"✅ Found: \$img\"
              break
            fi
            if [ \$attempt -eq 3 ]; then
              echo \"❌ ERROR: Image not found after 3 attempts: \$img\"
              docker logout '${CI_REGISTRY}'
              exit 1
            fi
            echo \"Image not found yet (attempt \$attempt/3), retrying in 10s...\"
            sleep 10
          done
        done

        docker logout '${CI_REGISTRY}'
      "

      # 3. Setup Env & Deploy (Fire and Forget)
      ssh "$SSH_HOST" bash -s <<DEPLOY_SCRIPT
        set -euo pipefail
        STAGING_DEPLOY_DIR="${STAGING_DEPLOY_DIR}"
        cd "\$STAGING_DEPLOY_DIR"

        mkdir -p infra
        # Write base secrets
        echo '${PROD_ENV_FILE}' | base64 -d > infra/.env.staging

        # Append CI-specific variables to .env.staging so the detached script can use them
        # Append CI-specific variables using grouped echo to avoid heredoc indentation issues
        {
          echo "REGISTRY_PASSWORD=${REGISTRY_PASSWORD}"
          echo "REGISTRY_USER=${REGISTRY_USER}"
          echo "CI_REGISTRY=${CI_REGISTRY}"
          echo "DOCKER_IMAGE_API=${DOCKER_IMAGE_API}"
          echo "DOCKER_IMAGE_WORKER=${DOCKER_IMAGE_WORKER}"
          echo "DOCKER_IMAGE_FRONTEND=${DOCKER_IMAGE_FRONTEND}"
          echo "DOCKER_IMAGE_BACKUP=${DOCKER_IMAGE_BACKUP}"
          echo "DEPLOY_COMMIT_SHA=${CI_COMMIT_SHA}"
          echo "BACKUP_PREFIX=staging_"
          echo "USE_PREBUILT_IMAGES=1"
          echo "DEPLOY_ENV=staging"
        } >> infra/.env.staging
        chmod 600 infra/.env.staging
        chmod +x ./infra/scripts/*.sh

        # Trigger detached deployment
        nohup ./infra/scripts/ci_deploy_trigger.sh \
            "./infra/.env.staging" \
            "./infra/scripts/deploy_staging.sh" \
            "last_deploy.log" > /dev/null 2>&1 &

        echo "✅ Deployment triggered in background. Check 'last_deploy.log' on server for details."
      DEPLOY_SCRIPT

verify_staging:
  stage: staging
  image: alpine:3.20
  needs: [deploy_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  script:
    - apk add --no-cache curl bind-tools openssh-client
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - |
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > ~/.ssh/id_rsa
      fi
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
    - |
      cat > ~/.ssh/config <<EOF
      Host *
        User $SSH_USER
        Port ${SSH_PORT:-22}
        ServerAliveInterval 15
        IdentityFile ~/.ssh/id_rsa
      EOF
    - chmod 600 ~/.ssh/config
    - |
      # Resolve SSH_HOST to IP and add staging subdomain to /etc/hosts
      SERVER_IP=$(nslookup "$SSH_HOST" | awk '/^Address: / { print $2 }' | tail -1)
      if [ -z "$SERVER_IP" ]; then
        echo "ERROR: Could not resolve IP for $SSH_HOST"
        exit 1
      fi
      echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
      echo "Added /etc/hosts entry: $SERVER_IP staging.$SSH_HOST"
    - |
      echo "Verifying staging deployment..."
      echo ""
      # Quick container status check
      echo "=== Checking Container Status ==="
      ssh "$SSH_HOST" \
        "docker ps --filter 'name=subro.*staging' --format '{{.Names}}: {{.Status}}' || echo 'Could not check containers'"
      echo ""

      # Increased retry window for staging container startup
      MAX_ATTEMPTS=24  # 24 * 10s = 4 minutes (was 2 minutes)
      DEPLOY_MARKER_PATH="${STAGING_DEPLOY_DIR}/infra/.deploy_sha"

      for attempt in $(seq 1 $MAX_ATTEMPTS); do
        echo "=== Verification attempt $attempt/$MAX_ATTEMPTS ==="

        MARKER_OK=false
        DEPLOYED_SHA=$(ssh "$SSH_HOST" \
          "cat '$DEPLOY_MARKER_PATH' 2>/dev/null || true")
        if [ -z "$DEPLOYED_SHA" ]; then
          echo "Deploy marker not found yet (expected $CI_COMMIT_SHA)"
        elif [ "$DEPLOYED_SHA" != "$CI_COMMIT_SHA" ]; then
          echo "Deploy marker mismatch (found $DEPLOYED_SHA, expected $CI_COMMIT_SHA)"
        else
          echo "Deploy marker matches expected commit."
          MARKER_OK=true
        fi

        HTTP_CODE=$(curl -k -s -o /dev/null -w "%{http_code}" --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "000")
        RESPONSE=$(curl -k -s --max-time 10 "https://staging.$SSH_HOST/api/v1/" || echo "")

        echo "HTTP Code: $HTTP_CODE"
        echo "Response: >>$RESPONSE<<"

        # Check if API is responding with valid content
        if [ "$MARKER_OK" = "true" ] && echo "$RESPONSE" | grep -qE '"(version|message)"'; then
          echo "✅ Staging API is responding correctly!"
          exit 0
        fi

        # Check for common error conditions
        if [ "$HTTP_CODE" = "502" ] || [ "$HTTP_CODE" = "503" ]; then
          echo "Containers not ready yet (HTTP $HTTP_CODE), waiting 10s..."
        elif [ "$HTTP_CODE" = "200" ] && [ -z "$RESPONSE" ]; then
          echo "Empty response - Caddy may not be routing to staging yet, waiting 10s..."
        else
          echo "Unexpected response, waiting 10s..."
        fi

        sleep 10
      done

      echo ""
      echo "❌ Staging verification failed after $MAX_ATTEMPTS attempts"
      echo ""
      echo "Possible causes:"
      echo "  - Staging containers failed to start"
      echo "  - Caddy not configured for staging.secure.go.ro"
      echo "  - Network/firewall issues"
      echo ""
      echo "Troubleshooting:"
      echo "  1. SSH to server and run: docker ps -a"
      echo "  2. Check API logs: docker logs subro-api-staging"
      echo "  3. Check Caddy logs: docker logs caddy"
      echo "  4. Test direct: curl http://localhost:8000/api/v1/"
      exit 1
  environment:
    name: staging
    url: https://staging.$SSH_HOST/

e2e_tests:
  stage: e2e
  image: mcr.microsoft.com/playwright:v1.57.0-jammy
  needs: [verify_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    E2E_BASE_URL: https://staging.$SSH_HOST
  script:
    - SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1) && echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
    - cd frontend
    - npm ci --prefer-offline
    - npx playwright install --with-deps chromium
    - npx playwright test --reporter=html
  artifacts:
    when: always
    paths:
      - frontend/playwright-report/
    expire_in: 7 days

# Cleanup staging after E2E tests (ephemeral staging)
cleanup_staging:
  stage: e2e
  image: alpine:3.20
  resource_group: staging
  needs:
    - job: e2e_tests
      artifacts: false
      optional: true
    - job: deploy_staging
      artifacts: false
      optional: true
    - job: verify_staging
      artifacts: false
      optional: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: always # Run even if e2e_tests fail
  extends: .ssh_setup
  script:
    - |
      ssh -o BatchMode=yes -o ServerAliveInterval=2 -o ServerAliveCountMax=30 -o IPQoS=0x00 -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" bash -s <<CLEANUP_SCRIPT
        set -euo pipefail

        # ANSI color codes for inline script
        BLUE='\033[0;34m'
        GREEN='\033[0;32m'
        NC='\033[0m'

        log() { echo -e "\${BLUE}[\$(date +'%Y-%m-%dT%H:%M:%S%z')]\${NC} \$*"; }
        success() { echo -e "\${GREEN}[\$(date +'%Y-%m-%dT%H:%M:%S%z')] ✓ \$*\${NC}"; }

        STAGING_DIR="\${STAGING_DEPLOY_DIR:-/opt/subro_web_staging}"

        log "Starting cleanup for staging environment..."
        log "Target directory: \$STAGING_DIR"

        if [ -d "\$STAGING_DIR" ]; then
          cd "\$STAGING_DIR"

          # Stop staging containers (app stack) and remove volumes for true ephemeral state
          docker compose -p subro_staging \
            --env-file infra/.env.staging \
            -f infra/docker/compose.prod.yml \
            down --remove-orphans --volumes 2>/dev/null || true

          success 'Staging environment stopped and cleaned'
        else
          log 'Staging directory not found, nothing to clean up'
        fi
      CLEANUP_SCRIPT
  allow_failure: true # Don't block pipeline if cleanup fails

# ============ DEPLOY STAGE ============
.deploy_template:
  stage: deploy
  image: alpine:3.20
  resource_group: $CI_ENVIRONMENT_NAME # Serializes deploys per environment
  needs:
    - job: backend_tests
      artifacts: false
    - job: scan_image_vulns
      artifacts: false
    - job: sast_scan
      artifacts: false
    - job: secret_scan
      artifacts: false
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - |
      if echo "$SSH_PRIVATE_KEY" | grep -q "BEGIN.*PRIVATE KEY"; then
        echo "$SSH_PRIVATE_KEY" > /tmp/ssh_key
      else
        echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
      fi
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  retry: 1

deploy_prod:
  extends: .deploy_template
  needs:
    # From template (Policy enforcement)
    - job: backend_tests
      artifacts: false
    - job: scan_image_vulns
      artifacts: false
    - job: sast_scan
      artifacts: false
    - job: secret_scan
      artifacts: false
    # Production specific
    - job: e2e_tests
      artifacts: false
    - job: build_api
      artifacts: false
    - job: build_worker
      artifacts: false
    - job: build_frontend
      artifacts: false
    - job: build_backup
      artifacts: false
  environment:
    name: production
    url: https://$SSH_HOST/
    deployment_tier: production
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
    - when: never
  script:
    - |
      # Deploy (Fire and Forget)
      ssh -o BatchMode=yes -o TCPKeepAlive=yes -o ConnectTimeout=30 \
          -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" bash -s <<DEPLOY_SCRIPT
        set -euo pipefail

        # Ensure deploy directory exists
        mkdir -p /opt/subro_web
        cd /opt/subro_web

        # Clone/Fetch repo (Synchronous part)
        if [ ! -d .git ]; then
          git clone --depth 1 "git@${CI_SERVER_HOST}:${CI_PROJECT_PATH}.git" .
        else
          git fetch --depth 1 origin "${CI_COMMIT_SHA}"
          git checkout -f "${CI_COMMIT_SHA}"
          git clean -fd
        fi

        # Ensure infra directory exists
        mkdir -p infra
        # Write base secrets
        echo '${PROD_ENV_FILE}' | base64 -d > infra/.env.prod

        # Append CI-specific variables
        # Append CI-specific variables
        {
          echo "REGISTRY_PASSWORD=${REGISTRY_PASSWORD}"
          echo "REGISTRY_USER=${REGISTRY_USER}"
          echo "CI_REGISTRY=${CI_REGISTRY}"
          echo "DOCKER_IMAGE_API=${DOCKER_IMAGE_API}"
          echo "DOCKER_IMAGE_WORKER=${DOCKER_IMAGE_WORKER}"
          echo "DOCKER_IMAGE_FRONTEND=${DOCKER_IMAGE_FRONTEND}"
          echo "DOCKER_IMAGE_BACKUP=${DOCKER_IMAGE_BACKUP}"
          echo "USE_PREBUILT_IMAGES=1"
        } >> infra/.env.prod
        chmod 600 infra/.env.prod
        chmod +x ./infra/scripts/*.sh

        # Trigger detached deployment
        nohup ./infra/scripts/ci_deploy_trigger.sh \
            "./infra/.env.prod" \
            "./infra/scripts/blue_green_deploy.sh" \
            "last_deploy_prod.log" > /dev/null 2>&1 &

        echo "✅ Production Deployment triggered in background."
      DEPLOY_SCRIPT

# ============ VERIFY STAGE ============
verify_prod:
  stage: verify
  image: alpine:3.20
  needs: [deploy_prod]
  environment:
    name: production
    url: https://$SSH_HOST/
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl
    - |
      echo "Verifying production deployment..."
      for i in $(seq 1 10); do
        RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/health" 2>&1 || true)
        if ! echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/api/v1/healthz" 2>&1 || true)
        fi
        if echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          echo "Production is healthy! Response: $RESPONSE"
          exit 0
        fi
        echo "Waiting for production ($i/10)... Response: $RESPONSE"
        sleep 5
      done
      echo "Production health check failed!"
      exit 1

# ============ RELEASE STAGE ============
release_version:
  stage: release
  image: alpine:3.20
  variables:
    GIT_STRATEGY: clone
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE !~ /chore\(release\): bump version/'
      when: on_success
  environment:
    name: production
    action: access
  extends: .ssh_setup
  before_script:
    - !reference [.ssh_setup, before_script]
    - apk add --no-cache git python3 py3-pip
    - pip install --break-system-packages tomlkit
    - git config --global user.email "ci-bot@gitlab.com"
    - git config --global user.name "CI Bot"
    - git remote set-url origin git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git
  script:
    - echo "Bumping version..."
    - python3 backend/scripts/bump_version.py

    # Check for changes
    - |
      if [[ -n $(git status --porcelain) ]]; then
        git add backend/pyproject.toml backend/app/core/config.py
        # Extract new version for commit message
        NEW_VERSION=$(grep 'version = ' backend/pyproject.toml | head -n 1 | cut -d '"' -f 2)
        git commit -m "chore(release): bump version to $NEW_VERSION"
        git push origin HEAD:$CI_COMMIT_BRANCH
        git push gitlab HEAD:$CI_COMMIT_BRANCH
        echo "Version bumped to $NEW_VERSION and pushed to origin and gitlab ($CI_COMMIT_BRANCH)"
      else
        echo "No version changes detected."
      fi

# ============ CLEANUP STAGE ============
cleanup_registry:
  stage: cleanup
  image: alpine:3.20
  variables:
    KEEP_TAGS: "10" # Number of recent tags to keep per repository
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl jq
    - |
      set -euo pipefail
      echo "=== Container Registry Cleanup ==="
      echo "Keeping last $KEEP_TAGS tags per repository"

      # Protected tags that should never be deleted
      PROTECTED_TAGS="latest main develop stable"

      # Helper: API call with retry
      api_call() {
        local url="$1"
        local method="${2:-GET}"
        for attempt in 1 2 3; do
          response=$(curl -s -w "\n%{http_code}" --request "$method" \
            --header "JOB-TOKEN: $CI_JOB_TOKEN" "$url" 2>/dev/null) || true
          http_code=$(echo "$response" | tail -1)
          body=$(echo "$response" | sed '$d')

          if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
            echo "$body"
            return 0
          fi
          echo "API call failed (attempt $attempt/3, HTTP $http_code)" >&2
          sleep 2
        done
        return 1
      }

      # Helper: Check if tag is protected
      is_protected() {
        local tag="$1"
        for protected in $PROTECTED_TAGS; do
          [ "$tag" = "$protected" ] && return 0
        done
        return 1
      }

      # Get all repositories
      REPOS=$(api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories" | \
        jq -r '.[] | "\(.id):\(.name)"' 2>/dev/null) || REPOS=""

      if [ -z "$REPOS" ]; then
        echo "No container repositories found or API error. Skipping cleanup."
        exit 0
      fi

      echo "Found repositories: $(echo "$REPOS" | cut -d: -f2 | tr '\n' ' ')"

      for repo_entry in $REPOS; do
        REPO_ID=$(echo "$repo_entry" | cut -d: -f1)
        REPO_NAME=$(echo "$repo_entry" | cut -d: -f2)
        echo ""
        echo "--- Processing: $REPO_NAME (ID: $REPO_ID) ---"

        # Fetch all tags with pagination (up to 200 tags)
        ALL_TAGS=""
        for page in 1 2; do
          PAGE_TAGS=$(api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$REPO_ID/tags?per_page=100&page=$page") || continue
          [ -z "$PAGE_TAGS" ] || [ "$PAGE_TAGS" = "[]" ] && break
          ALL_TAGS="$ALL_TAGS$PAGE_TAGS"
        done

        if [ -z "$ALL_TAGS" ] || [ "$ALL_TAGS" = "[]" ]; then
          echo "  No tags found, skipping."
          continue
        fi

        # Combine pages, sort by created_at (newest first), extract names
        SORTED_TAGS=$(echo "$ALL_TAGS" | jq -s 'add | sort_by(.created_at) | reverse | .[].name' -r 2>/dev/null) || continue
        TOTAL_TAGS=$(echo "$SORTED_TAGS" | wc -l)
        echo "  Found $TOTAL_TAGS tags"

        # Skip first N (keep them), delete the rest
        TAGS_TO_DELETE=$(echo "$SORTED_TAGS" | tail -n +$((KEEP_TAGS + 1)))
        DELETE_COUNT=$(echo "$TAGS_TO_DELETE" | grep -c . || echo 0)

        if [ "$DELETE_COUNT" -eq 0 ]; then
          echo "  Nothing to delete (only $TOTAL_TAGS tags, keeping $KEEP_TAGS)"
          continue
        fi

        echo "  Deleting $DELETE_COUNT old tags..."
        DELETED=0
        SKIPPED=0

        echo "$TAGS_TO_DELETE" | while read -r tag; do
          [ -z "$tag" ] && continue

          # Skip protected tags
          if is_protected "$tag"; then
            echo "    ⏭ Skipping protected: $tag"
            continue
          fi

          # Delete tag
          if api_call "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$REPO_ID/tags/$tag" "DELETE" >/dev/null 2>&1; then
            echo "    ✓ Deleted: $tag"
          else
            echo "    ✗ Failed: $tag"
          fi
        done

        echo "  Done with $REPO_NAME"
      done

      echo ""
      echo "=== Cleanup complete! ==="
  allow_failure: true

# ============ ROLLBACK (on failure) ============
rollback_prod:
  stage: verify
  image: alpine:3.20
  needs:
    - job: verify_prod
      optional: true # Required for when: on_failure to trigger
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_failure
  extends: .ssh_setup
  script:
    - |
      echo "Rolling back to previous version..."
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        cd /opt/subro_web
        chmod +x ./infra/scripts/*.sh
        if [ -f ./infra/scripts/rollback.sh ]; then
          ./infra/scripts/rollback.sh
        else
          echo 'Rollback script not found, manual intervention required'
          exit 1
        fi
      "
  environment:
    name: production
    action: stop
  allow_failure: true
