# ============================================================
# subro_web GitLab CI/CD Pipeline
# Based on project patterns, adapted for blue-green deploy
# ============================================================

stages:
  - pre
  - build
  - test
  - scan
  - staging
  - e2e
  - deploy
  - verify
  - release
  - notify
  - cleanup

# -------- Global Variables --------
variables:
  IMAGE_API: $CI_REGISTRY_IMAGE/api:$CI_COMMIT_SHORT_SHA
  IMAGE_WORKER: $CI_REGISTRY_IMAGE/worker:$CI_COMMIT_SHORT_SHA
  IMAGE_FRONTEND: $CI_REGISTRY_IMAGE/frontend:$CI_COMMIT_SHORT_SHA
  IMAGE_BACKUP: $CI_REGISTRY_IMAGE/backup:$CI_COMMIT_SHORT_SHA
  DOCKER_TLS_CERTDIR: ""

# -------- Docker-in-Docker Helper --------
.default-docker:
  image: docker:25
  services:
    - name: docker:25-dind
      command: ["--tls=false"]
  variables:
    DOCKER_HOST: tcp://docker:2375
  before_script:
    - set -euo pipefail
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"

# -------- DEBUG (optional) --------
ci_debug_env:
  stage: pre
  image: alpine:3.20
  rules:
    - when: always
  allow_failure: true
  script:
    - 'echo "Pipeline: $CI_PIPELINE_SOURCE | Branch: $CI_COMMIT_BRANCH | SHA: $CI_COMMIT_SHORT_SHA"'

# ============ BUILD STAGE ============
.build_template:
  stage: build
  extends: .default-docker
  variables:
    DOCKER_BUILDKIT: "1"
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"
  rules:
    - if: "$CI_COMMIT_BRANCH"
  retry: 1

build_api:
  extends: .build_template
  variables:
    SERVICE: api
    IMAGE: $IMAGE_API
    TARGET: production-api
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend

build_worker:
  extends: .build_template
  variables:
    SERVICE: worker
    IMAGE: $IMAGE_WORKER
    TARGET: production-worker
    DOCKERFILE: backend/Dockerfile
    CONTEXT: backend

build_frontend:
  extends: .build_template
  variables:
    SERVICE: frontend
    IMAGE: $IMAGE_FRONTEND
    TARGET: production
    DOCKERFILE: frontend/Dockerfile
    CONTEXT: frontend

build_backup:
  extends: .build_template
  variables:
    SERVICE: backup
    IMAGE: $IMAGE_BACKUP
    TARGET: production
    DOCKERFILE: infra/docker/backup/Dockerfile
    CONTEXT: .
  script:
    - |
      set -euo pipefail
      docker buildx create --name builder --use 2>/dev/null || docker buildx use builder
      docker buildx inspect --bootstrap
      docker buildx build \
        --push \
        -t "$IMAGE" \
        -t "$CI_REGISTRY_IMAGE/$SERVICE:latest" \
        --target "$TARGET" \
        --build-arg VITE_API_BASE_URL="${VITE_API_BASE_URL:-/api/v1}" \
        --build-arg VITE_WS_BASE_URL="${VITE_WS_BASE_URL:-/api/v1}" \
        --build-arg VITE_SETUP_TOKEN="${VITE_SETUP_TOKEN:-}" \
        --cache-to=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE",mode=max \
        --cache-from=type=registry,ref="$CI_REGISTRY_IMAGE/cache-$SERVICE" \
        -f "$DOCKERFILE" \
        "$CONTEXT"

# ============ TEST STAGE ============
backend_tests:
  stage: test
  extends: .default-docker
  needs: [build_api]
  variables:
    # Database config for test container
    POSTGRES_USER: admin
    POSTGRES_PASSWORD: "Pa44w0rd"
    POSTGRES_DB: subappdb
    POSTGRES_DB_TEST: subappdb_pytest
  script:
    - set -euo pipefail
    - docker pull "$IMAGE_API"
    - docker network create testnet || true

    # Start dependencies (Production-like setup)
    - docker run -d --name db --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB postgres:16-alpine
    - docker run --rm --network testnet --entrypoint python $IMAGE_API -c "import kombu, redis; print('DEBUG VERSIONS kombu', kombu.__version__, 'redis', redis.__version__)"
    - docker run -d --name db_test --network testnet -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_DB=$POSTGRES_DB_TEST postgres:16-alpine
    - docker run -d --name redis --network testnet redis:7-alpine

    # Wait for databases to be ready
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet postgres:16-alpine pg_isready -h db -U $POSTGRES_USER && \
           docker run --rm --network testnet postgres:16-alpine pg_isready -h db_test -U $POSTGRES_USER; then
          echo "Databases ready!"
          break
        fi
        echo "Waiting for Databases ($i/30)..." && sleep 2
      done

    # Start API with env file from repo, override network-specific vars
    - |
      docker run -d --name api --network testnet \
        --env-file backend/.env.test \
        -e DATABASE_URL="postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}" \
        -e DB_HOST="db" \
        -e POSTGRES_SERVER="db" \
        -e REDIS_HOST="redis" \
        -e CELERY_BROKER_URL="redis://redis:6379/0" \
        -e CELERY_RESULT_BACKEND="redis://redis:6379/1" \
        -e REDIS_PUBSUB_URL="redis://redis:6379/2" \
        -e TEST_DATABASE_HOST="db_test" \
        -e TEST_DATABASE_PORT="5432" \
        -e TEST_API_BASE_URL="http://localhost:8000" \
        -e TEST_WS_BASE_URL="ws://localhost:8000" \
        -v "$(pwd)/backend/tests:/app/tests" \
        "$IMAGE_API"

    # Wait for API to be healthy
    - |
      for i in $(seq 1 30); do
        if docker run --rm --network testnet curlimages/curl:8.7.1 -fsS http://api:8000/health 2>/dev/null; then
          echo "API healthy!"
          break
        fi
        echo "Waiting for API ($i/30)..." && sleep 2
      done

    # Install test dependencies and run tests
    - docker exec api poetry install --with dev
    - docker exec api poetry run pytest -p no:cacheprovider tests/
  after_script:
    - docker logs api 2>&1 | tail -50 || true
    - docker rm -f api db db_test redis || true

frontend_tests:
  stage: test
  image: node:20-alpine
  needs: []
  variables:
    NPM_CONFIG_CACHE: frontend/.npm
    VITE_API_BASE_URL: "${VITE_API_BASE_URL:-http://localhost:8000/api/v1}"
    VITE_WS_BASE_URL: "${VITE_WS_BASE_URL:-ws://localhost:8000/api/v1}"
    VITE_SETUP_TOKEN: "${VITE_SETUP_TOKEN:-test-setup-token}"
    VITE_API_PROXY_TARGET: "${VITE_API_PROXY_TARGET:-http://localhost:8000}"
  cache:
    key:
      files:
        - frontend/package-lock.json
    policy: pull-push
    paths:
      - frontend/node_modules/
      - frontend/.npm/
  before_script:
    - set -euo pipefail
    - cd frontend
    - npm config set cache .npm
  script:
    - npm ci --prefer-offline --no-audit --no-fund
    - npm test

# ============ SCAN STAGE ============
scan_image_vulns:
  stage: scan
  image:
    name: aquasec/trivy:0.53.0
    entrypoint: [""]
  needs: [build_api, build_worker, build_frontend]
  variables:
    TRIVY_SEVERITY: "HIGH,CRITICAL"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        TRIVY_SEVERITY: "CRITICAL"
    - when: on_success
  cache:
    key: trivy-cache
    paths: [.trivycache/]
  script:
    - |
      set -euo pipefail
      export TRIVY_USERNAME="$CI_REGISTRY_USER"
      export TRIVY_PASSWORD="$CI_REGISTRY_PASSWORD"
      for img in "$IMAGE_API" "$IMAGE_FRONTEND" "$IMAGE_WORKER"; do
        echo "Scanning $img..."
        trivy image --cache-dir .trivycache --exit-code 1 \
          --severity "$TRIVY_SEVERITY" --timeout 10m --no-progress \
          --skip-files "/app/secrets/google-api-config.json" \
          "$img"
      done
  allow_failure: false
  retry: 1

sast_scan:
  stage: scan
  image:
    name: returntocorp/semgrep:latest
    entrypoint: [""]
  variables:
    SAST_MIN_SEVERITY: "WARNING"
    SAST_FAIL_ON_FINDINGS: "false"
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      variables:
        SAST_MIN_SEVERITY: "ERROR"
        SAST_FAIL_ON_FINDINGS: "true"
    - when: on_success
  script:
    - |
      EXTRA_ARGS=""
      if [ "$SAST_FAIL_ON_FINDINGS" == "true" ]; then
        EXTRA_ARGS="--error"
      fi
      semgrep scan --config auto --sarif -o semgrep.sarif \
        --severity "$SAST_MIN_SEVERITY" $EXTRA_ARGS "$CI_PROJECT_DIR"
  artifacts:
    reports:
      sast: semgrep.sarif
    when: always
  allow_failure: false

secret_scan:
  stage: scan
  image:
    name: aquasec/trivy:0.53.0
    entrypoint: [""]
  script:
    - |
      trivy fs --scanners secret \
        --skip-dirs ".git,__pycache__,.venv,venv,node_modules,.ruff_cache,.pytest_cache" \
        --exit-code 1 --no-progress --timeout 5m .
  allow_failure: false

# ============ STAGING STAGE ============
deploy_staging:
  stage: staging
  image: alpine:3.20
  resource_group: staging
  variables:
    STAGING_DEPLOY_DIR: /opt/subro_web_staging
  needs:
    - job: backend_tests
    - job: scan_image_vulns
    - job: sast_scan
    - job: secret_scan
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  script:
    - |
      set -euo pipefail
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        set -euo pipefail

        STAGING_DEPLOY_DIR=\"\${STAGING_DEPLOY_DIR:-/opt/subro_web_staging}\"
        echo \"Using staging deploy dir: \$STAGING_DEPLOY_DIR\"

        # Ensure staging directory exists
        mkdir -p \"\$STAGING_DEPLOY_DIR\"
        cd \"\$STAGING_DEPLOY_DIR\"

        # Clone or update repo
        if [ ! -d .git ]; then
          git clone --depth 1 git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git .
        else
          git fetch --depth 1 origin $CI_COMMIT_SHA
          git checkout -f $CI_COMMIT_SHA
          git clean -fd
        fi
        echo \"Checked out commit: \$(git rev-parse HEAD)\"

        # Setup staging environment
        mkdir -p infra
        echo '$PROD_ENV_FILE' | base64 -d > infra/.env.staging
        chmod 600 infra/.env.staging

        # Login to registry
        echo '$REGISTRY_PASSWORD' | docker login -u '$REGISTRY_USER' --password-stdin $CI_REGISTRY

        # Set image tags
        export DOCKER_IMAGE_API='$IMAGE_API'
        export DOCKER_IMAGE_WORKER='$IMAGE_WORKER'
        export DOCKER_IMAGE_FRONTEND='$IMAGE_FRONTEND'
        export DOCKER_IMAGE_BACKUP='$IMAGE_BACKUP'
        export BACKUP_PREFIX=staging_
        export USE_PREBUILT_IMAGES=1
        export DEPLOY_ENV=staging

        # Deploy staging
        chmod +x ./infra/scripts/*.sh
        ./infra/scripts/deploy_staging.sh

        docker logout $CI_REGISTRY
      "
  environment:
    name: staging
    url: https://staging.$SSH_HOST/
  retry: 1

verify_staging:
  stage: staging
  image: alpine:3.20
  needs: [deploy_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  script:
    - apk add --no-cache curl
    - |
      # Resolve SSH_HOST to IP and add staging subdomain to /etc/hosts
      SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1)
      if [ -z "$SERVER_IP" ]; then
        echo "ERROR: Could not resolve IP for $SSH_HOST"
        exit 1
      fi
      echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
      echo "Added /etc/hosts entry: $SERVER_IP staging.$SSH_HOST"
    - |
      echo "Verifying staging deployment..."
      for i in $(seq 1 10); do
        HEALTH_TMP=$(mktemp)
        HTTP_CODE=$(curl -k --silent --show-error --max-time 10 -o "$HEALTH_TMP" -w "%{http_code}" "https://staging.$SSH_HOST/health" || true)
        RESPONSE=$(cat "$HEALTH_TMP" 2>/dev/null || true)
        rm -f "$HEALTH_TMP"
        echo "Health /health code=$HTTP_CODE body=${RESPONSE:-<empty>}"

        if ! echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          HEALTH_TMP=$(mktemp)
          HTTP_CODE=$(curl -k --silent --show-error --max-time 10 -o "$HEALTH_TMP" -w "%{http_code}" "https://staging.$SSH_HOST/api/v1/healthz" || true)
          RESPONSE=$(cat "$HEALTH_TMP" 2>/dev/null || true)
          rm -f "$HEALTH_TMP"
          echo "Health /api/v1/healthz code=$HTTP_CODE body=${RESPONSE:-<empty>}"
        fi

        if echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          echo "Staging is healthy! Response: $RESPONSE"
          exit 0
        fi
        echo "Waiting for staging ($i/10)... Response: $RESPONSE"
        sleep 5
      done
      echo "Staging health check failed!"
      exit 1
  environment:
    name: staging
    url: https://staging.$SSH_HOST/

# ============ E2E TESTS STAGE ============
e2e_tests:
  stage: e2e
  image: mcr.microsoft.com/playwright:v1.40.0-jammy
  needs: [verify_staging]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  variables:
    E2E_BASE_URL: https://staging.$SSH_HOST
  script:
    - SERVER_IP=$(getent hosts "$SSH_HOST" | awk '{print $1}' | head -1) && echo "$SERVER_IP staging.$SSH_HOST" >> /etc/hosts
    - cd frontend
    - npm ci --prefer-offline
    - npx playwright install --with-deps chromium
    - npx playwright test --reporter=html
  artifacts:
    when: always
    paths:
      - frontend/playwright-report/
    expire_in: 7 days

# ============ DEPLOY STAGE ============
.deploy_template:
  stage: deploy
  image: alpine:3.20
  resource_group: $CI_ENVIRONMENT_NAME # Serializes deploys per environment
  needs:
    - job: backend_tests
    - job: scan_image_vulns
    - job: sast_scan
    - job: secret_scan
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  retry: 1

deploy_prod:
  extends: .deploy_template
  needs:
    - job: e2e_tests
      artifacts: false
  environment:
    name: production
    url: https://$SSH_HOST/
    deployment_tier: production
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
    - when: never
  script:
    - |
      set -euo pipefail
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        set -euo pipefail

        # Ensure deploy directory exists (first-time setup)
        mkdir -p /opt/subro_web
        cd /opt/subro_web

        # Clone repo if first deploy (infra scripts needed)
        if [ ! -d .git ]; then
          git clone --depth 1 git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git .
        else
          git fetch --depth 1 origin $CI_COMMIT_SHA
          git checkout -f $CI_COMMIT_SHA
          git clean -fd
        fi

        # Ensure infra directory exists and decode .env.prod
        mkdir -p infra
        echo '$PROD_ENV_FILE' | base64 -d > infra/.env.prod
        chmod 600 infra/.env.prod

        # Login to registry
        echo '$REGISTRY_PASSWORD' | docker login -u '$REGISTRY_USER' --password-stdin $CI_REGISTRY

        # Set image tags
        export DOCKER_IMAGE_API='$IMAGE_API'
        export DOCKER_IMAGE_WORKER='$IMAGE_WORKER'
        export DOCKER_IMAGE_FRONTEND='$IMAGE_FRONTEND'
        export DOCKER_IMAGE_BACKUP='$IMAGE_BACKUP'
        export USE_PREBUILT_IMAGES=1

        # Ensure scripts are executable
        chmod +x ./infra/scripts/*.sh

        # Deploy
        ./infra/scripts/blue_green_deploy.sh

        # Cleanup
        docker logout $CI_REGISTRY
      "

# ============ VERIFY STAGE ============
verify_prod:
  stage: verify
  image: alpine:3.20
  needs: [deploy_prod]
  environment:
    name: production
    url: https://$SSH_HOST/
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl
    - |
      echo "Verifying production deployment..."
      for i in $(seq 1 10); do
        RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/health" 2>&1 || true)
        if ! echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          RESPONSE=$(curl --fail --silent --max-time 10 "https://$SSH_HOST/api/v1/healthz" 2>&1 || true)
        fi
        if echo "$RESPONSE" | grep -qE '"status":\s*"(ok|healthy)"'; then
          echo "Production is healthy! Response: $RESPONSE"
          exit 0
        fi
        echo "Waiting for production ($i/10)... Response: $RESPONSE"
        sleep 5
      done
      echo "Production health check failed!"
      exit 1

# ============ RELEASE STAGE ============
release_version:
  stage: release
  image: alpine:3.20
  variables:
    GIT_STRATEGY: clone
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_MESSAGE !~ /chore\(release\): bump version/'
      when: on_success
  environment:
    name: production
    action: access
  before_script:
    - set -euo pipefail
    - apk add --no-cache git openssh-client openssh-keygen bash python3 py3-pip
    - pip install --break-system-packages tomlkit
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan "$CI_SERVER_HOST" >> ~/.ssh/known_hosts
    - git config --global user.email "ci-bot@gitlab.com"
    - git config --global user.name "CI Bot"
    - git remote set-url origin git@$CI_SERVER_HOST:$CI_PROJECT_PATH.git
  script:
    - echo "Bumping version..."
    - python3 backend/scripts/bump_version.py

    # Check for changes
    - |
      if [[ -n $(git status --porcelain) ]]; then
        git add backend/pyproject.toml backend/app/core/config.py
        # Extract new version for commit message
        NEW_VERSION=$(grep 'version = ' backend/pyproject.toml | head -n 1 | cut -d '"' -f 2)
        git commit -m "chore(release): bump version to $NEW_VERSION"
        git push origin HEAD:$CI_COMMIT_BRANCH
        git push gitlab HEAD:$CI_COMMIT_BRANCH
        echo "Version bumped to $NEW_VERSION and pushed to origin and gitlab ($CI_COMMIT_BRANCH)"
      else
        echo "No version changes detected."
      fi

# ============ NOTIFY STAGE ============
notify_success:
  stage: notify
  image: alpine:3.20
  needs:
    - job: verify_prod
    - job: release_version
      optional: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $SLACK_WEBHOOK_URL'
      when: on_success
  script:
    - apk add --no-cache curl
    - |
      VERSION=$(grep 'version = ' backend/pyproject.toml | head -n 1 | cut -d '"' -f 2 || echo "unknown")
      curl -X POST "$SLACK_WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"‚úÖ *Deploy successful*\\nüì¶ $CI_PROJECT_NAME v$VERSION\\nüîó <$CI_PIPELINE_URL|Pipeline #$CI_PIPELINE_ID>\"}"
  allow_failure: true

notify_failure:
  stage: notify
  image: alpine:3.20
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $SLACK_WEBHOOK_URL'
      when: on_failure
  script:
    - apk add --no-cache curl
    - |
      curl -X POST "$SLACK_WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{\"text\":\"‚ùå *Deploy FAILED*\\nüì¶ $CI_PROJECT_NAME\\nüîó <$CI_PIPELINE_URL|Pipeline #$CI_PIPELINE_ID>\"}"
  allow_failure: true

# ============ CLEANUP STAGE ============
cleanup_registry:
  stage: cleanup
  image: alpine:3.20
  needs:
    - job: notify_success
      optional: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_success
  script:
    - apk add --no-cache curl jq
    - |
      echo "Cleaning up old container images..."
      # Keep last 10 tags, delete older ones
      # This uses GitLab Container Registry API
      for repo in api worker frontend; do
        echo "Checking $repo repository..."
        TAGS=$(curl -s --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
          "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories" | \
          jq -r ".[] | select(.name==\"$repo\") | .id" || true)

        if [ -n "$TAGS" ]; then
          echo "Found repository ID: $TAGS for $repo"
          # List and delete old tags (keeping last 10)
          curl -s --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
            "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$TAGS/tags" | \
            jq -r '.[10:] | .[].name' | while read tag; do
              echo "Deleting old tag: $tag"
              curl -s --request DELETE --header "PRIVATE-TOKEN: $CI_JOB_TOKEN" \
                "$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories/$TAGS/tags/$tag" || true
            done
        fi
      done
      echo "Cleanup complete!"
  allow_failure: true

# ============ ROLLBACK (on failure) ============
rollback_prod:
  stage: verify
  image: alpine:3.20
  needs: [verify_prod]
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: on_failure
  before_script:
    - set -euo pipefail
    - apk add --no-cache bash openssh-client
    - eval "$(ssh-agent -s)"
    - echo "$SSH_PRIVATE_KEY" | base64 -d > /tmp/ssh_key
    - chmod 600 /tmp/ssh_key
    - ssh-add /tmp/ssh_key
    - rm -f /tmp/ssh_key
    - mkdir -p ~/.ssh
    - ssh-keyscan -p "${SSH_PORT:-22}" "$SSH_HOST" >> ~/.ssh/known_hosts
  script:
    - |
      echo "Rolling back to previous version..."
      ssh -p "${SSH_PORT:-22}" "$SSH_USER@$SSH_HOST" "
        cd /opt/subro_web
        chmod +x ./infra/scripts/*.sh
        if [ -f ./infra/scripts/rollback.sh ]; then
          ./infra/scripts/rollback.sh
        else
          echo 'Rollback script not found, manual intervention required'
          exit 1
        fi
      "
  environment:
    name: production
    action: stop
  allow_failure: true
